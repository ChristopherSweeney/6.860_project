{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JndnmDMp66FL"
   },
   "source": [
    "#### Copyright 2018 Google LLC\n",
    "\n",
    "This library is free software; you can redistribute it and/or\n",
    "modify it under the terms of the GNU Lesser General Public\n",
    "License as published by the Free Software Foundation; either\n",
    "version 2.1 of the License, or (at your option) any later version.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "hMqWDc_m6rUC"
   },
   "outputs": [],
   "source": [
    "# This library is free software; you can redistribute it and/or\n",
    "# modify it under the terms of the GNU Lesser General Public\n",
    "# License as published by the Free Software Foundation; either\n",
    "# version 2.1 of the License, or (at your option) any later version.\n",
    "# \n",
    "# This library is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\n",
    "# Lesser General Public License for more details.\n",
    "# \n",
    "# You should have received a copy of the GNU Lesser General Public\n",
    "# License along with this library; if not, write to the Free Software\n",
    "# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EuA1RKib8_98"
   },
   "source": [
    "# Mitigating Unwanted Biases with Adversarial Learning\n",
    "\n",
    "Authors: Andrew Zaldivar, Ben Hutchinson, Blake Lemoine, Brian Zhang, Margaret Mitchell\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1x0PRJii9C33"
   },
   "source": [
    "## Summary of this Notebook\n",
    "\n",
    "This notebook is a guide to the paper ([archiv](https://arxiv.org/pdf/1801.07593.pdf))\n",
    "\n",
    "> ```Brian Zhang, Blake Lemoine and Margaret Mitchell. Mitigating Unwanted Biases with Adversarial Learning. AAAI Conference on AI, Ethics and Society, 2018.```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8BnQsDuO9FSt"
   },
   "source": [
    "## Intro statement of problem\n",
    "\n",
    "Embeddings are a powerful mechanism for projecting a discrete variable (e.g. words, locales, urls) into a multi-dimensional real valued space.  Several strong methods have been developed for learning embeddings.  One example is the [Skipgram](http://www.cs.brandeis.edu/~marc/misc/proceedings/lrec-2006/pdf/357_pdf.pdf) algorithm.  In that algorithm the surrounding context is used to predict the presence of a word.  Unfortunately, much real world textual data has subtle bias that machine learning algorithms will implicitly include in the embeddings created from that data.  This bias can be illustrated by performing a word analogy task using the learned embeddings.\n",
    "\n",
    "It is worth noting that the usages of terms like _fair_ and _bias_ are used in this notebook in the context of a particular definition of fairness sometimes referred to as \"Demographic Parity\" or \"Equality of Outcomes\" ([Hardt et. al 2016](http://papers.nips.cc/paper/6373-equality-of-opportunity-in-supervised-learning)).  This definition of fairness effectively says that any relationship at all between a variable of interest and a _protected variable_ is an example of unwanted bias.  Other definitions of fairness such as \"Equality of Odds\" can be employed when there is believed to be some form of proper relationship between the variable of interest and the protected variable.  However, all uses of _fair_ and _bias_ here should be interpreted in the context of \"Demographic Parity\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G50xVJvB92z9"
   },
   "source": [
    "First, we'll import all the packages that we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1322
    },
    "colab_type": "code",
    "id": "gOrI3fGc87cz",
    "outputId": "43769798-0a70-4e65-b8a2-2d6cf531d2fd"
   },
   "outputs": [],
   "source": [
    "# !pip install -U gensim~=3.2.0\n",
    "import gensim\n",
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "# !pip install --upgrade-strategy=only-if-needed tensorflow~=1.6.0rc0\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qrGSp0fA9W8B"
   },
   "source": [
    "Now we'll sync the data for the colab to a tmp directory from Google Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "F_B4Madl_5xp",
    "outputId": "f6bebc8a-9b9f-4d47-ed76-71587e99dd9d"
   },
   "outputs": [],
   "source": [
    "\n",
    "local_dir_name = 'data'\n",
    "\n",
    "WORD2VEC_FILE = os.path.join(local_dir_name+\"/embeddings\", \"GoogleNews-vectors-negative300.bin.gz\")\n",
    "ANALOGIES_FILE = os.path.join(local_dir_name, \"questions-words.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ztVqGJ8xGF1b"
   },
   "outputs": [],
   "source": [
    "def load_word2vec_format(f, max_num_words=None):\n",
    "  \"\"\"Loads word2vec data from a file handle.\n",
    "\n",
    "  Similar to gensim.models.keyedvectors.KeyedVectors.load_word2vec_format\n",
    "  but takes a file handle as input rather than a filename. This lets us use\n",
    "  GFile. Also only accepts binary files.\n",
    "\n",
    "  Args:\n",
    "    f: file handle\n",
    "    max_num_words: number of words to load. If None, load all.\n",
    "\n",
    "  Returns:\n",
    "    Word2vec data as keyedvectors.EuclideanKeyedVectors.\n",
    "  \"\"\"\n",
    "  header = f.readline()\n",
    "  vocab_size, vector_size = (\n",
    "      int(x) for x in header.rstrip().split())  # throws for invalid file format\n",
    "  print \"vector_size =  %d\" % vector_size\n",
    "  result = gensim.models.keyedvectors.EuclideanKeyedVectors()\n",
    "  num_words = 0\n",
    "  result.vector_size = vector_size\n",
    "  result.syn0 = np.zeros((vocab_size, vector_size), dtype=np.float32)\n",
    "  \n",
    "  def add_word(word, weights):\n",
    "    word_id = len(result.vocab)\n",
    "    if word in result.vocab:\n",
    "      print(\"duplicate word '%s', ignoring all but first\", word)\n",
    "      return\n",
    "    result.vocab[word] = gensim.models.keyedvectors.Vocab(\n",
    "        index=word_id, count=vocab_size - word_id)\n",
    "    result.syn0[word_id] = weights\n",
    "    result.index2word.append(word)\n",
    "\n",
    "  if max_num_words and max_num_words < vocab_size:\n",
    "    num_embeddings = max_num_words\n",
    "  else:\n",
    "    num_embeddings = vocab_size\n",
    "  print \"Loading %d embeddings\" % num_embeddings\n",
    "  \n",
    "  binary_len = np.dtype(np.float32).itemsize * vector_size\n",
    "  for _ in xrange(vocab_size):\n",
    "    # mixed text and binary: read text first, then binary\n",
    "    word = []\n",
    "    while True:\n",
    "      ch = f.read(1)\n",
    "      if ch == b' ':\n",
    "        break\n",
    "      if ch == b'':\n",
    "        raise EOFError(\"unexpected end of input; is count incorrect or file otherwise damaged?\")\n",
    "      if ch != b'\\n':  # ignore newlines in front of words (some binary files have)\n",
    "        word.append(ch)\n",
    "    word = gensim.utils.to_unicode(b''.join(word), encoding='utf-8', errors='strict')\n",
    "    weights = np.frombuffer(f.read(binary_len), dtype=np.float32)\n",
    "    add_word(word, weights)\n",
    "    num_words = num_words + 1\n",
    "    if max_num_words and num_words == max_num_words:\n",
    "      break\n",
    "  if result.syn0.shape[0] != len(result.vocab):\n",
    "    print(\n",
    "        \"duplicate words detected, shrinking matrix size from %i to %i\",\n",
    "        result.syn0.shape[0], len(result.vocab))\n",
    "  result.syn0 = np.ascontiguousarray(result.syn0[:len(result.vocab)])\n",
    "  assert (len(result.vocab), vector_size) == result.syn0.shape\n",
    "\n",
    "  print(\"loaded %s matrix\", result.syn0.shape)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "UNAARuwKGHu5",
    "outputId": "58d70bb8-935e-4c9c-e95e-29ffeee47a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word embeddings from data/embeddings/GoogleNews-vectors-negative300.bin.gz\n",
      "vector_size =  300\n",
      "Loading 2000000 embeddings\n",
      "('duplicate words detected, shrinking matrix size from %i to %i', 3000000, 2000000)\n",
      "('loaded %s matrix', (2000000, 300))\n",
      "CPU times: user 48.5 s, sys: 1.68 s, total: 50.2 s\n",
      "Wall time: 51.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initialize the embeddings client if this hasn't been done yet.\n",
    "# For the efficiency of this notebook we just load the first 2M words, and don't\n",
    "# re-initialize the client if it already exists. You could of course filter the\n",
    "# word list in other ways.\n",
    "if not 'client' in vars():\n",
    "  print \"Loading word embeddings from %s\" % WORD2VEC_FILE\n",
    "  with gzip.GzipFile(fileobj=open(WORD2VEC_FILE, 'r')) as f:\n",
    "    client = load_word2vec_format(f, max_num_words=2000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y_N47nk687UG"
   },
   "source": [
    "The following blocks load a data file with analogy training examples and displays some of them as examples.  By changing the indices selected in the final block you can change which analogies from the training set are being displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pKGlGo5VJnU4"
   },
   "outputs": [],
   "source": [
    "def print_knn(client, v, k):\n",
    "  print \"%d closest neighbors to A-B+C:\" % k\n",
    "  for neighbor, score in client.similar_by_vector(\n",
    "      v.flatten().astype(float), topn=k):\n",
    "    print \"%s : score=%f\" % (neighbor, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eG_tc8kOkMlm"
   },
   "source": [
    "Let's take a look at the analogies that the model generates for *man*:*woman*::*boss*:$\\underline{\\quad}$.\n",
    "Try changing ``\"boss\"`` to ``\"friend\"`` to see further examples of problematic analogies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "6rEfwqtDIt0Q",
    "outputId": "48205f75-b9a3-49ad-8e42-8c41e20be3f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 closest neighbors to A-B+C:\n",
      "boss : score=0.814025\n",
      "bosses : score=0.558710\n",
      "manageress : score=0.499752\n",
      "coworker : score=0.471055\n",
      "receptionist : score=0.470017\n"
     ]
    }
   ],
   "source": [
    "# Use a word embedding to compute an analogy\n",
    "# Edit the parameters below to get different analogies\n",
    "A = \"man\"\n",
    "B = \"woman\"\n",
    "C = \"boss\"\n",
    "NUM_ANALOGIES = 5\n",
    "\n",
    "in_arr = []\n",
    "for i, word in enumerate((A, B, C)):\n",
    "  in_arr.append(client.word_vec(word))\n",
    "in_arr = np.array([in_arr])\n",
    "\n",
    "print_knn(client, -in_arr[0, 0, :] + in_arr[0, 1, :] + in_arr[0, 2, :],\n",
    "          NUM_ANALOGIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CLhtOKQKKN4W"
   },
   "outputs": [],
   "source": [
    "def load_analogies(filename):\n",
    "  \"\"\"Loads analogies.\n",
    "\n",
    "  Args:\n",
    "    filename: the file containing the analogies.\n",
    "\n",
    "  Returns:\n",
    "    A list containing the analogies.\n",
    "  \"\"\"\n",
    "  analogies = []\n",
    "  with open(filename, \"r\") as fast_file:\n",
    "    for line in fast_file:\n",
    "      line = line.strip()\n",
    "      # in the analogy file, comments start with :\n",
    "      if line[0] == \":\":\n",
    "        continue\n",
    "      words = line.split()\n",
    "      # there are no misformatted lines in the analogy file, so this should\n",
    "      # only happen once we're done reading all analogies.\n",
    "      if len(words) != 4:\n",
    "        print \"Invalid line: %s\" % line\n",
    "        continue\n",
    "      analogies.append(words)\n",
    "  print \"loaded %d analogies\" % len(analogies)\n",
    "  return analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "2RtfKMshE0gy",
    "outputId": "c806fe01-7bc4-4252-d7d2-3edc2b73f336"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 19544 analogies\n",
      "Athens is to Greece as Baghdad is to Iraq\n",
      "Athens is to Greece as Bangkok is to Thailand\n",
      "Athens is to Greece as Beijing is to China\n",
      "Athens is to Greece as Berlin is to Germany\n",
      "Athens is to Greece as Bern is to Switzerland\n",
      "Athens is to Greece as Cairo is to Egypt\n",
      "Athens is to Greece as Canberra is to Australia\n",
      "Athens is to Greece as Hanoi is to Vietnam\n",
      "Athens is to Greece as Havana is to Cuba\n",
      "Athens is to Greece as Helsinki is to Finland\n",
      "Athens is to Greece as Islamabad is to Pakistan\n",
      "Athens is to Greece as Kabul is to Afghanistan\n",
      "Athens is to Greece as London is to England\n",
      "Athens is to Greece as Madrid is to Spain\n",
      "Athens is to Greece as Moscow is to Russia\n",
      "Athens is to Greece as Oslo is to Norway\n",
      "Athens is to Greece as Ottawa is to Canada\n",
      "Athens is to Greece as Paris is to France\n",
      "Athens is to Greece as Rome is to Italy\n",
      "Athens is to Greece as Stockholm is to Sweden\n"
     ]
    }
   ],
   "source": [
    "analogies = load_analogies(ANALOGIES_FILE)\n",
    "print \"\\n\".join(\"%s is to %s as %s is to %s\" % tuple(x) for x in analogies[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w4LQ8JcJKXmU"
   },
   "source": [
    "## Adversarial Networks for Bias Mitigation\n",
    "\n",
    "The method presented here for removing some of the bias from embeddings is based on the idea that those embeddings are intended to be used to predict some outcome $Y$ based on an input $X$ but that outcome should, in a fair world, be completely unrelated to some protected variable $Z$.  If that were the case then knowing $Y$ would not help you predict $Z$ any better than chance.  This principle can be directly translated into two networks in series as illustrated below.  The first attempts to predict $Y$ using $X$ as input.  The second attempts to use the predicted value of $Y$ to predict $Z$.  See Figure 1 of [the paper](https://arxiv.org/pdf/1801.07593.pdf).\n",
    "\n",
    "However, simply training the weights in W based on $\\nabla_WL_1$ and the weights in $U$ based on $\\nabla_UL_2$ won’t actually achieve an unbiased model.  In order to do that you need to incorporate into $W$’s update function the concept that $U$ should be no better than chance at predicting $Z$.  The way that you can achieve that is analogous to how Generative Adversarial Networks (GANs) ([Goodfellow et al. 2014](http://papers.nips.cc/paper/5423-generative-adversarial-nets)) train their generators.\n",
    "\n",
    "In addition to $\\nabla_WL_1$ you incorporate the negation of $\\nabla_WL_2$ into $W$’s update function.  However, it’s possible that $\\nabla_WL_1$ is changing $W$ in a way which will improve accuracy by using the biased information you are trying to protect.  In order to avoid that you also incorporate a term which removes that component of $\\nabla_WL_1$ by projecting it onto $\\nabla_WL_2$.  Once you’ve incorporated those two terms, the update function for $W$ becomes:\n",
    "\n",
    "\n",
    "$\\nabla_WL_1-proj_{(\\nabla_WL_2)}\\nabla_WL_1 - \\nabla_WL_2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kwfqHFzAKgiV"
   },
   "source": [
    "### Defining the Protected Variable of Embeddings\n",
    "\n",
    "The description of how to incorporate adversarial networks into machine learned models above is very generic because the technique is generally applicable for any type of systems which can be described in terms of input $X$ being predictive of $Y$ but potentially containing information about a protected variable $Z$.  So long as you can construct the relevant update functions you can apply this technique.  However, that doesn’t tell you much about the nature of $X$, $Y$ and $Z$.  In the case of the word analogies task above, $X = B + C - A$ and $Y = D$.  Figuring out what $Z$ should be is a little bit trickier though.  For that we can look to a paper by [Bulokbasi et. al.](http://papers.nips.cc/paper/6227-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings) where they developed an unsupervised methodology for removing gendered semantics from word embeddings.\n",
    "\n",
    "The first step is to select pairs of words which are relevant to the type of bias you are trying to remove.  In the case of gender you can choose word pairs like “man”:”woman” and “boy”:girl” which have gender as the only difference in their semantics.  Once you have these word pairs you can compute the difference between their embeddings to produce vectors in the embeddings’ semantic space which are roughly parallel to the semantics of gender.  Performing Principal Components Analysis (PCA) on those vectors then gives you the major components of the semantics of gender as defined by the gendered word pairs provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pM8NTb7bKo_5"
   },
   "outputs": [],
   "source": [
    "def _np_normalize(v):\n",
    "  \"\"\"Returns the input vector, normalized.\"\"\"\n",
    "  return v / np.linalg.norm(v)\n",
    "\n",
    "\n",
    "def load_vectors(client, analogies):\n",
    "  \"\"\"Loads and returns analogies and embeddings.\n",
    "\n",
    "  Args:\n",
    "    client: the client to query.\n",
    "    analogies: a list of analogies.\n",
    "\n",
    "  Returns:\n",
    "    A tuple with:\n",
    "    - the embedding matrix itself\n",
    "    - a dictionary mapping from strings to their corresponding indices\n",
    "      in the embedding matrix\n",
    "    - the list of words, in the order they are found in the embedding matrix\n",
    "  \"\"\"\n",
    "  words_unfiltered = set()\n",
    "  for analogy in analogies:\n",
    "    words_unfiltered.update(analogy)\n",
    "  print \"found %d unique words\" % len(words_unfiltered)\n",
    "\n",
    "  vecs = []\n",
    "  words = []\n",
    "  index_map = {}\n",
    "  for word in words_unfiltered:\n",
    "    try:\n",
    "      vecs.append(_np_normalize(client.word_vec(word)))\n",
    "      index_map[word] = len(words)\n",
    "      words.append(word)\n",
    "    except KeyError:\n",
    "      print \"word not found: %s\" % word\n",
    "  print \"words not filtered out: %d\" % len(words)\n",
    "\n",
    "  return np.array(vecs), index_map, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "6TLg0wygKKW0",
    "outputId": "95102bb6-7227-4ffe-ca02-792d3981ecdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 905 unique words\n",
      "words not filtered out: 905\n",
      "word embedding dimension: 300\n"
     ]
    }
   ],
   "source": [
    "embed, indices, words = load_vectors(client, analogies)\n",
    "\n",
    "embed_dim = len(embed[0].flatten())\n",
    "print \"word embedding dimension: %d\" % embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFSGOaqDLPij"
   },
   "outputs": [],
   "source": [
    "def find_gender_direction(embed,\n",
    "                          indices):\n",
    "  \"\"\"Finds and returns a 'gender direction'.\"\"\"\n",
    "  pairs = [\n",
    "      (\"woman\", \"man\"),\n",
    "      (\"her\", \"his\"),\n",
    "      (\"she\", \"he\"),\n",
    "      (\"aunt\", \"uncle\"),\n",
    "      (\"niece\", \"nephew\"),\n",
    "      (\"daughters\", \"sons\"),\n",
    "      (\"mother\", \"father\"),\n",
    "      (\"daughter\", \"son\"),\n",
    "      (\"granddaughter\", \"grandson\"),\n",
    "      (\"girl\", \"boy\"),\n",
    "      (\"stepdaughter\", \"stepson\"),\n",
    "      (\"mom\", \"dad\"),\n",
    "  ]\n",
    "  m = []\n",
    "  for wf, wm in pairs:\n",
    "    m.append(embed[indices[wf]] - embed[indices[wm]])\n",
    "  m = np.array(m)\n",
    "\n",
    "  # the next three lines are just a PCA.\n",
    "  m = np.cov(np.array(m).T)\n",
    "  evals, evecs = np.linalg.eig(m)\n",
    "  return _np_normalize(np.real(evecs[:, np.argmax(evals)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1320
    },
    "colab_type": "code",
    "id": "fSj8daFnKvNr",
    "outputId": "c9de79bb-13ea-49fd-95b5-933dad3f286a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender direction: [-7.94102556e-02 -8.62907447e-02 -9.36046086e-02 -6.82094699e-02\n",
      "  2.10405600e-02  9.09854549e-03  3.69967887e-02  6.76862493e-03\n",
      " -1.35619514e-01  3.62402266e-02  6.62573091e-02 -9.91207519e-04\n",
      "  4.59588196e-02 -6.13280986e-02 -2.14388384e-03 -1.15234663e-02\n",
      " -7.85432247e-02 -6.24935300e-02 -8.79221813e-02 -1.73189498e-02\n",
      "  1.72767315e-02  1.72292006e-03 -3.53863093e-02  5.24758140e-03\n",
      "  1.45531278e-02  7.97479425e-03 -3.40466363e-02 -2.83068019e-02\n",
      " -1.02746018e-01 -1.78703381e-01 -1.88934471e-02 -7.23408473e-02\n",
      " -1.06853722e-01 -4.48557103e-02 -5.89368281e-03 -7.94341237e-02\n",
      "  1.17833895e-03 -5.86551743e-02  3.10732848e-02 -3.12044830e-03\n",
      " -4.15907969e-02 -7.17691663e-03 -2.56242105e-02 -3.41547309e-02\n",
      " -6.38683437e-02  7.83023380e-02  1.41320146e-03 -1.92862106e-02\n",
      "  4.27428205e-02  6.23987501e-03  2.58495545e-02 -2.53252181e-02\n",
      " -6.58270803e-03  6.81094699e-02  2.29536006e-02 -3.01465541e-02\n",
      " -8.76620455e-03 -3.48142318e-03 -1.03283556e-02 -4.43229749e-02\n",
      "  4.82072717e-02 -1.38947109e-01 -5.49295845e-02  1.42456083e-01\n",
      " -3.82443506e-02  5.43604495e-02  6.42165093e-02  1.80136131e-01\n",
      " -2.95630757e-02 -7.22149244e-02  1.53964050e-02  3.28869800e-02\n",
      "  4.24435462e-02  2.30966085e-02  1.12612003e-02  4.36423822e-02\n",
      " -7.29707357e-04 -3.64472450e-02  7.50969536e-02  7.15724467e-02\n",
      " -2.49016159e-02 -3.88707157e-02  2.47988546e-02  8.84891062e-03\n",
      " -1.57366197e-01  5.67419234e-02 -5.13816362e-02  1.39329337e-02\n",
      " -2.49230367e-03 -8.79131791e-03 -6.95209856e-02 -9.16509235e-02\n",
      " -5.41995965e-02  1.73404339e-02 -6.23635240e-03  1.97633141e-02\n",
      " -1.39783648e-02 -2.09286823e-02  5.42969581e-02  5.43749286e-02\n",
      " -2.15415313e-02 -2.41145593e-03  4.85206319e-02 -5.10697951e-02\n",
      " -1.34003908e-02  2.13284548e-02 -1.12171602e-01 -3.89264077e-02\n",
      "  7.07023362e-02  6.22944837e-03 -2.23500522e-02 -5.02853247e-02\n",
      "  9.33401918e-02  2.40351194e-02 -4.21102256e-02  5.20160292e-02\n",
      "  4.75287371e-02 -8.11995185e-02 -7.38389538e-03  2.29741450e-02\n",
      "  7.62025009e-03  5.25651699e-02  1.24644176e-01 -3.22192007e-02\n",
      "  9.16384819e-02  1.89994690e-01  7.22935579e-03  5.25197973e-02\n",
      "  8.30303086e-02  6.13005472e-02  4.64472574e-02  1.97908008e-02\n",
      " -5.19907059e-02  1.51321854e-01  2.47586642e-02  7.32033143e-02\n",
      "  2.25787357e-02 -5.42248469e-02 -6.66883855e-02 -1.05413345e-01\n",
      " -2.27129787e-02 -8.88976863e-02 -3.63838620e-02 -8.98777237e-02\n",
      "  5.52357322e-03  3.68328109e-02 -6.02720613e-02  7.25324488e-02\n",
      " -6.96779923e-03 -1.40505525e-01  2.02638063e-03 -3.35120036e-04\n",
      "  7.33190982e-02 -6.88252971e-02  4.61441135e-02  7.68183901e-02\n",
      " -3.30161505e-02  8.79513077e-02  7.73010673e-02  9.25715579e-03\n",
      " -9.04122538e-02  7.52339842e-03 -5.72907134e-02  4.28152602e-02\n",
      "  2.86397606e-02 -5.62480396e-02 -3.87299617e-02 -3.87058619e-02\n",
      "  5.85371538e-02  9.43237537e-02  2.71958769e-02  5.25680701e-02\n",
      "  2.08041380e-02 -3.83978501e-02 -1.10333866e-01 -1.88310924e-02\n",
      "  1.85601924e-02  1.11566914e-01 -3.58018442e-04  2.06100921e-02\n",
      "  8.06141199e-03 -1.22897665e-02 -2.16353311e-02  8.96372961e-03\n",
      "  1.00744761e-02 -2.87074612e-02  1.11815260e-02  5.87513547e-02\n",
      " -5.30042040e-02  1.27520561e-01  1.25319079e-02 -1.03226821e-01\n",
      " -1.97484407e-02 -3.42320863e-02  1.05197274e-01  1.76135002e-02\n",
      "  2.80467006e-02 -8.31193905e-03  1.10601081e-02 -1.40255927e-02\n",
      " -4.74642660e-02 -2.75716115e-03 -1.01467816e-01 -5.44390020e-02\n",
      " -8.95335277e-02 -1.65279682e-02 -1.12849845e-01  3.43099446e-02\n",
      "  1.07627595e-02 -1.95155440e-02  8.49218426e-03  7.18273491e-02\n",
      "  9.43337430e-02  5.93970694e-02  4.42150488e-02  3.93104455e-03\n",
      "  1.55634159e-02  1.47405106e-02  9.86230904e-02 -3.51293172e-02\n",
      " -5.48265317e-03  4.39100322e-02  1.00173280e-01  4.73382845e-02\n",
      " -8.75985652e-02 -8.01811478e-02 -2.84562342e-02 -1.62217727e-02\n",
      " -5.23065164e-02  2.49845262e-02 -3.01654996e-02  1.45479843e-02\n",
      " -1.35092122e-02 -1.05868228e-02  1.30148387e-04  1.41828245e-03\n",
      "  9.64396967e-02 -1.37944939e-03  1.28795558e-02  5.14631073e-02\n",
      "  4.12590524e-02 -1.05768228e-01  4.51128868e-02  7.65950631e-02\n",
      "  8.35771929e-02  5.04364947e-02 -7.37407299e-02 -1.27911956e-02\n",
      " -7.96467866e-02  1.42361577e-02  2.28048101e-02 -2.45567372e-02\n",
      "  1.51617365e-02 -8.14192396e-02 -2.99888041e-02 -9.38109909e-03\n",
      "  9.73554448e-03  2.77240676e-02 -1.44856280e-02 -2.15535602e-02\n",
      "  2.22796961e-02 -2.44437577e-02 -1.73757435e-04 -5.92882308e-02\n",
      " -6.75396381e-02 -6.88861185e-03  1.79634043e-02  5.94912151e-02\n",
      "  1.34743549e-02  3.44577542e-02 -5.31342611e-02  1.22593336e-02\n",
      "  6.63454886e-03  1.72750748e-01  8.17975838e-02  7.98060175e-03\n",
      "  3.48464502e-02  3.15494960e-02  3.00879598e-02  1.26706401e-02\n",
      " -5.09294800e-02 -1.49508612e-02  5.45480741e-03  5.96511080e-02\n",
      "  8.97118066e-03  2.17049865e-02 -5.54851264e-02  4.33567631e-03\n",
      "  4.53374907e-02  9.27058295e-02 -2.28239338e-02  3.36064502e-02\n",
      " -5.63768244e-02 -4.46595961e-02  2.17583523e-03 -3.09787628e-02\n",
      " -1.12975804e-02  9.51682266e-02  3.74056898e-02  1.14501412e-01]\n"
     ]
    }
   ],
   "source": [
    "# Using the embeddings, find the gender vector.\n",
    "gender_direction = find_gender_direction(embed, indices)\n",
    "print \"gender direction: %s\" % str(gender_direction.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pVJIOmh2LcV7"
   },
   "source": [
    "Once you have the first principal component of the embedding differences, you can start projecting the embeddings of words onto it.  That projection is roughly the degree to which a word is relevant to the latent protected variable defined by the first principle component of the word pairs given.  This projection can then be taken as the protected variable $Z$ which the adversary is attempting to predict on the basis of the predicted value of $Y$.  The code below illustrates how to construct a function which computes $Z$ from $X$ in this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FpOpO6RGBznY"
   },
   "source": [
    "Try editing the WORD param in the next cell to see the projection of other words onto the gender direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BipW3yjvLJFz",
    "outputId": "3f3c0ebe-2733-463e-9a6b-4e9d6a62b0b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.706431491878112\n"
     ]
    }
   ],
   "source": [
    "WORD = \"she\"\n",
    "\n",
    "word_vec = client.word_vec(WORD)\n",
    "print word_vec.dot(gender_direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RlxOV3RZBpou"
   },
   "source": [
    "Let's now look at the words with the largest *negative* projection onto the gender dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "rxjWqgCXLe6S",
    "outputId": "c50559f6-d01a-45f2-a4ef-05859092e6bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             word  gender_score\n",
      "188           his     -0.660903\n",
      "843            he     -0.584860\n",
      "284  unimpressive     -0.544877\n",
      "615       Anaheim     -0.503144\n",
      "535         Libya     -0.486664\n",
      "242       playing     -0.472259\n",
      "754          play     -0.459254\n",
      "362      sharpest     -0.455905\n",
      "830       Detroit     -0.454509\n",
      "303        calmly     -0.448576\n"
     ]
    }
   ],
   "source": [
    "words = set()\n",
    "for a in analogies:\n",
    "  words.update(a)\n",
    "\n",
    "df = pd.DataFrame(data={\"word\": list(words)})\n",
    "df[\"gender_score\"] = df[\"word\"].map(\n",
    "    lambda w: client.word_vec(w).dot(gender_direction))\n",
    "df.sort_values(by=\"gender_score\", inplace=True)\n",
    "print df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v9fx985BByiU"
   },
   "source": [
    "Let's now look at the words with the largest *positive* projection onto the gender dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "JTLLr12vB2mN",
    "outputId": "a9174598-802a-496e-ddf5-8b41483fb2c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            word  gender_score\n",
      "520      husband      0.950914\n",
      "462  policewoman      0.816518\n",
      "28         women      0.758530\n",
      "733          mom      0.732802\n",
      "335     princess      0.719736\n",
      "517          she      0.706431\n",
      "355      sisters      0.699692\n",
      "429      stepson      0.686814\n",
      "813          her      0.683995\n",
      "470        queen      0.682354\n"
     ]
    }
   ],
   "source": [
    "df.sort_values(by=\"gender_score\", inplace=True, ascending=False)\n",
    "print df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rMEGkmSlTsR6"
   },
   "source": [
    "### Training the model\n",
    "\n",
    "Training adversarial networks is hard. They are touchy, and if touched the wrong way, they blow up VERY quickly. One must be very careful to train both models slowly enough, so that the parameters in the models do not diverge. In practice, this usually entails significantly lowering the step size of both the classifier and the adversary. It is also probably beneficial to initialize the parameters of the adversary to be extremely small, to ensure that the classifier does not overfit against a particular (sub-optimal) adversary (such overfitting can very quickly cause divergence!).  It is also possible that if the classifier is too good at hiding the protected variable from the adversary then the adversary will impose updates that diverge in an effort to improve its performance.  The solution to that can sometimes be to actually increase the adversary’s learning rate to prevent divergence (something almost unheard of in most learning systems).  Below is a system for learning the debiasing model for word embeddings described above.  \n",
    "\n",
    "Inspect the terminal output for the kernel to inspect the performance of the model. Try modifying the hyperparameters at the top to see how that impacts the convergence properties of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7tsGeQLKT3ZY"
   },
   "outputs": [],
   "source": [
    "def tf_normalize(x):\n",
    "  \"\"\"Returns the input vector, normalized.\n",
    "\n",
    "  A small number is added to the norm so that this function does not break when\n",
    "  dealing with the zero vector (e.g. if the weights are zero-initialized).\n",
    "\n",
    "  Args:\n",
    "    x: the tensor to normalize\n",
    "  \"\"\"\n",
    "  return x / (tf.norm(x) + np.finfo(np.float32).tiny)\n",
    "\n",
    "\n",
    "class AdversarialEmbeddingModel(object):\n",
    "  \"\"\"A model for doing adversarial training of embedding models.\"\"\"\n",
    "\n",
    "  def __init__(self, client,\n",
    "               data_p, embed_dim, projection,\n",
    "               projection_dims, pred):\n",
    "    \"\"\"Creates a new AdversarialEmbeddingModel.\n",
    "\n",
    "    Args:\n",
    "      client: The (possibly biased) embeddings.\n",
    "      data_p: Placeholder for the data.\n",
    "      embed_dim: Number of dimensions used in the embeddings.\n",
    "      projection: The space onto which we are \"projecting\".\n",
    "      projection_dims: Number of dimensions of the projection.\n",
    "      pred: Prediction layer.\n",
    "    \"\"\"\n",
    "    # load the analogy vectors as well as the embeddings\n",
    "    self.client = client\n",
    "    self.data_p = data_p\n",
    "    self.embed_dim = embed_dim\n",
    "    self.projection = projection\n",
    "    self.projection_dims = projection_dims\n",
    "    self.pred = pred\n",
    "\n",
    "  def nearest_neighbors(self, sess, in_arr,\n",
    "                        k):\n",
    "    \"\"\"Finds the nearest neighbors to a vector.\n",
    "\n",
    "    Args:\n",
    "      sess: Session to use.\n",
    "      in_arr: Vector to find nearest neighbors to.\n",
    "      k: Number of nearest neighbors to return\n",
    "    Returns:\n",
    "      List of up to k pairs of (word, score).\n",
    "    \"\"\"\n",
    "    v = sess.run(self.pred, feed_dict={self.data_p: in_arr})\n",
    "    return self.client.similar_by_vector(v.flatten().astype(float), topn=k)\n",
    "\n",
    "  def write_to_file(self, sess, f):\n",
    "    \"\"\"Writes a model to disk.\"\"\"\n",
    "    np.savetxt(f, sess.run(self.projection))\n",
    "\n",
    "  def read_from_file(self, sess, f):\n",
    "    \"\"\"Reads a model from disk.\"\"\"\n",
    "    loaded_projection = np.loadtxt(f).reshape(\n",
    "        [self.embed_dim, self.projection_dims])\n",
    "    sess.run(self.projection.assign(loaded_projection))\n",
    "\n",
    "  def fit(self,\n",
    "          sess,\n",
    "          data,\n",
    "          data_p,\n",
    "          labels,\n",
    "          labels_p,\n",
    "          protect,\n",
    "          protect_p,\n",
    "          gender_direction,\n",
    "          pred_learning_rate,\n",
    "          protect_learning_rate,\n",
    "          protect_loss_weight,\n",
    "          num_steps,\n",
    "          batch_size,\n",
    "          debug_interval=1000):\n",
    "    \"\"\"Trains a model.\n",
    "\n",
    "    Args:\n",
    "      sess: Session.\n",
    "      data: Features for the training data.\n",
    "      data_p: Placeholder for the features for the training data.\n",
    "      labels: Labels for the training data.\n",
    "      labels_p: Placeholder for the labels for the training data.\n",
    "      protect: Protected variables.\n",
    "      protect_p: Placeholder for the protected variables.\n",
    "      gender_direction: The vector from find_gender_direction().\n",
    "      pred_learning_rate: Learning rate for predicting labels.\n",
    "      protect_learning_rate: Learning rate for protecting variables.\n",
    "      protect_loss_weight: The constant 'alpha' found in\n",
    "          debias_word_embeddings.ipynb.\n",
    "      num_steps: Number of training steps.\n",
    "      batch_size: Number of training examples in each step.\n",
    "      debug_interval: Frequency at which to log performance metrics during\n",
    "          training.\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "        data_p: data,\n",
    "        labels_p: labels,\n",
    "        protect_p: protect,\n",
    "    }\n",
    "    # define the prediction loss\n",
    "    pred_loss = tf.losses.mean_squared_error(labels_p, self.pred)\n",
    "\n",
    "    # compute the prediction of the protected variable.\n",
    "    # The \"trainable\"/\"not trainable\" designations are for the predictor. The\n",
    "    # adversary explicitly specifies its own list of weights to train.\n",
    "    protect_weights = tf.get_variable(\n",
    "        \"protect_weights\", [self.embed_dim, 1], trainable=False)\n",
    "    protect_pred = tf.matmul(self.pred, protect_weights)\n",
    "    protect_loss = tf.losses.mean_squared_error(protect_p, protect_pred)\n",
    "\n",
    "    pred_opt = tf.train.AdamOptimizer(pred_learning_rate)\n",
    "    protect_opt = tf.train.AdamOptimizer(protect_learning_rate)\n",
    "\n",
    "    protect_grad = {v: g for (g, v) in pred_opt.compute_gradients(protect_loss)}\n",
    "    pred_grad = []\n",
    "\n",
    "    # applies the gradient expression found in the document linked\n",
    "    # at the top of this file.\n",
    "    for (g, v) in pred_opt.compute_gradients(pred_loss):\n",
    "      unit_protect = tf_normalize(protect_grad[v])\n",
    "      # the two lines below can be commented out to train without debiasing\n",
    "      g -= tf.reduce_sum(g * unit_protect) * unit_protect\n",
    "      g -= protect_loss_weight * protect_grad[v]\n",
    "      pred_grad.append((g, v))\n",
    "      pred_min = pred_opt.apply_gradients(pred_grad)\n",
    "\n",
    "    # compute the loss of the protected variable prediction.\n",
    "    protect_min = protect_opt.minimize(protect_loss, var_list=[protect_weights])\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    step = 0\n",
    "    while step < num_steps:\n",
    "      # pick samples at random without replacement as a minibatch\n",
    "      ids = np.random.choice(len(data), batch_size, False)\n",
    "      data_s, labels_s, protect_s = data[ids], labels[ids], protect[ids]\n",
    "      sgd_feed_dict = {\n",
    "          data_p: data_s,\n",
    "          labels_p: labels_s,\n",
    "          protect_p: protect_s,\n",
    "      }\n",
    "\n",
    "      if not step % debug_interval:\n",
    "        metrics = [pred_loss, protect_loss, self.projection]\n",
    "        metrics_o = sess.run(metrics, feed_dict=feed_dict)\n",
    "        pred_loss_o, protect_loss_o, proj_o = metrics_o\n",
    "        # log stats every so often: number of steps that have passed,\n",
    "        # prediction loss, adversary loss\n",
    "        print(\"step: %d; pred_loss_o: %f; protect_loss_o: %f\" % (step,\n",
    "                     pred_loss_o, protect_loss_o))\n",
    "        for i in range(proj_o.shape[1]):\n",
    "          print(\"proj_o: %f; dot(proj_o, gender_direction): %f)\" %\n",
    "                       (np.linalg.norm(proj_o[:, i]),\n",
    "                       np.dot(proj_o[:, i].flatten(), gender_direction)))\n",
    "      sess.run([pred_min, protect_min], feed_dict=sgd_feed_dict)\n",
    "      step += 1\n",
    "      \n",
    "def filter_analogies(analogies,\n",
    "                     index_map):\n",
    "  filtered_analogies = []\n",
    "  for analogy in analogies:\n",
    "    if filter(index_map.has_key, analogy) != analogy:\n",
    "      print \"at least one word missing for analogy: %s\" % analogy\n",
    "    else:\n",
    "      filtered_analogies.append(map(index_map.get, analogy))\n",
    "  return filtered_analogies\n",
    "\n",
    "def make_data(\n",
    "    analogies, embed,\n",
    "    gender_direction):\n",
    "  \"\"\"Preps the training data.\n",
    "\n",
    "  Args:\n",
    "    analogies: a list of analogies\n",
    "    embed: the embedding matrix from load_vectors\n",
    "    gender_direction: the gender direction from find_gender_direction\n",
    "\n",
    "  Returns:\n",
    "    Three numpy arrays corresponding respectively to the input, output, and\n",
    "    protected variables.\n",
    "  \"\"\"\n",
    "  data = []\n",
    "  labels = []\n",
    "  protect = []\n",
    "  for analogy in analogies:\n",
    "    # the input is just the word embeddings of the first three words\n",
    "    data.append(embed[analogy[:3]])\n",
    "    # the output is just the word embeddings of the last word\n",
    "    labels.append(embed[analogy[3]])\n",
    "    # the protected variable is the gender component of the output embedding.\n",
    "    # the extra pair of [] is so that the array has the right shape after\n",
    "    # it is converted to a numpy array.\n",
    "    protect.append([np.dot(embed[analogy[3]], gender_direction)])\n",
    "  # Convert all three to numpy arrays, and return them.\n",
    "  return tuple(map(np.array, (data, labels, protect)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2DeRq6L9BiTo"
   },
   "source": [
    "Edit the training parameters below to experiment with different training runs.\n",
    "\n",
    "For example, try increasing the number of training steps to 50k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "GhOH62wAR-wz",
    "outputId": "87269f8a-1152-4eec-a16e-9a4190074588"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0; pred_loss_o: 0.003394; protect_loss_o: 0.015218\n",
      "proj_o: 1.424954; dot(proj_o, gender_direction): 0.063502)\n",
      "step: 1000; pred_loss_o: 0.003397; protect_loss_o: 0.011169\n",
      "proj_o: 1.433486; dot(proj_o, gender_direction): 0.097062)\n",
      "step: 2000; pred_loss_o: 0.003431; protect_loss_o: 0.010139\n",
      "proj_o: 1.537885; dot(proj_o, gender_direction): 0.128413)\n",
      "step: 3000; pred_loss_o: 0.003491; protect_loss_o: 0.010036\n",
      "proj_o: 1.672662; dot(proj_o, gender_direction): 0.152525)\n",
      "step: 4000; pred_loss_o: 0.003553; protect_loss_o: 0.009335\n",
      "proj_o: 1.781189; dot(proj_o, gender_direction): 0.168080)\n",
      "step: 5000; pred_loss_o: 0.003590; protect_loss_o: 0.007902\n",
      "proj_o: 1.846500; dot(proj_o, gender_direction): 0.176368)\n",
      "step: 6000; pred_loss_o: 0.003602; protect_loss_o: 0.006302\n",
      "proj_o: 1.873674; dot(proj_o, gender_direction): 0.175678)\n",
      "step: 7000; pred_loss_o: 0.003593; protect_loss_o: 0.004959\n",
      "proj_o: 1.867117; dot(proj_o, gender_direction): 0.170912)\n",
      "step: 8000; pred_loss_o: 0.003570; protect_loss_o: 0.003973\n",
      "proj_o: 1.834275; dot(proj_o, gender_direction): 0.166546)\n",
      "step: 9000; pred_loss_o: 0.003540; protect_loss_o: 0.003267\n",
      "proj_o: 1.782190; dot(proj_o, gender_direction): 0.163636)\n"
     ]
    }
   ],
   "source": [
    "# Edit the training parameters below to experiment with different training runs.\n",
    "# For example, try \n",
    "pred_learning_rate = 2**-16\n",
    "protect_learning_rate = 2**-16\n",
    "protect_loss_weight = 1.0\n",
    "num_steps = 10000\n",
    "batch_size = 1000\n",
    "\n",
    "embed_dim = 300\n",
    "projection_dims = 1\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "with tf.variable_scope('var_scope', reuse=tf.AUTO_REUSE):\n",
    "    analogy_indices = filter_analogies(analogies, indices)\n",
    "\n",
    "    data, labels, protect = make_data(analogy_indices, embed, gender_direction)\n",
    "    data_p = tf.placeholder(tf.float32, shape=[None, 3, embed_dim], name=\"data\")\n",
    "    labels_p = tf.placeholder(tf.float32, shape=[None, embed_dim], name=\"labels\")\n",
    "    protect_p = tf.placeholder(tf.float32, shape=[None, 1], name=\"protect\")\n",
    "\n",
    "    # projection is the space onto which we are \"projecting\". By default, this is\n",
    "    # one-dimensional, but this can be tuned by projection_dims\n",
    "    projection = tf.get_variable(\"projection\", [embed_dim, projection_dims])\n",
    "\n",
    "    # build the prediction layer\n",
    "    # pred is the simple computation of d = -a + b + c for a : b :: c : d\n",
    "    pred = -data_p[:, 0, :] + data_p[:, 1, :] + data_p[:, 2, :]\n",
    "    pred -= tf.matmul(tf.matmul(pred, projection), tf.transpose(projection))\n",
    "\n",
    "    trained_model = AdversarialEmbeddingModel(\n",
    "        client, data_p, embed_dim, projection, projection_dims, pred)\n",
    "\n",
    "    trained_model.fit(sess, data, data_p, labels, labels_p, protect, protect_p, gender_direction,\n",
    "              pred_learning_rate,\n",
    "            protect_learning_rate, protect_loss_weight, num_steps, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vt--_4UCWTIg"
   },
   "source": [
    "### Analogy generation using the embeddings with bias reduced by the adversarial model\n",
    "\n",
    "Let's see how the model that has been trained to mitigate bias performs on the analogy task.\n",
    "As before, change \"boss\" to \"friend\" to see how those analogies have changed too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "_56sPusFUVQP",
    "outputId": "e1b2db68-f87f-4123-aaab-d094302f6c5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 closest neighbors to A-B+C:\n",
      "boss : score=0.785314\n",
      "bosses : score=0.559288\n",
      "exec : score=0.488238\n",
      "supremo : score=0.454166\n",
      "honcho : score=0.453955\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "A = \"man\"\n",
    "B = \"woman\"\n",
    "C = \"boss\"\n",
    "NUM_ANALOGIES = 5\n",
    "\n",
    "# Use a word embedding to compute an analogy\n",
    "in_arr = []\n",
    "for i, word in enumerate((A, B, C)):\n",
    "  in_arr.append(client.word_vec(word))\n",
    "in_arr = np.array([in_arr])\n",
    "\n",
    "print_knn(client, sess.run(pred, feed_dict={data_p: in_arr}),\n",
    "          NUM_ANALOGIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Ei_ioWT_DDa"
   },
   "source": [
    "##Conclusion\n",
    "\n",
    "The method demonstrated here helps to reduce the amount of bias in word embeddings and, although not demonstrated here, generalizes quite well to other domains and tasks.  By trying to hide a protected variable from an adversary, a machine learned system can reduce the amount of biased information about that protected variable implicit in the system.  In addition to the specific method demonstrated here there are many variations on this theme which can be used to achieve different degrees and types of debiasing.  For example, you could debias with respect to more than one principle component of the protected variable by having the adverary predict multiple projections.  Many other elaborations on this basic idea are possible and hopefully this relatively simple system can serve as the basis for more complex and sophisticated systems capable of achieving subtle types of bias mitigation in many applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "JndnmDMp66FL"
   ],
   "name": "Mitigating Unwanted Biases in Word Embeddings with Adversarial Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
