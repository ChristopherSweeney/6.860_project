{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JndnmDMp66FL"
   },
   "source": [
    "#### Copyright 2018 Google LLC\n",
    "\n",
    "This library is free software; you can redistribute it and/or\n",
    "modify it under the terms of the GNU Lesser General Public\n",
    "License as published by the Free Software Foundation; either\n",
    "version 2.1 of the License, or (at your option) any later version.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "hMqWDc_m6rUC"
   },
   "outputs": [],
   "source": [
    "# This library is free software; you can redistribute it and/or\n",
    "# modify it under the terms of the GNU Lesser General Public\n",
    "# License as published by the Free Software Foundation; either\n",
    "# version 2.1 of the License, or (at your option) any later version.\n",
    "# \n",
    "# This library is distributed in the hope that it will be useful,\n",
    "# but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU\n",
    "# Lesser General Public License for more details.\n",
    "# \n",
    "# You should have received a copy of the GNU Lesser General Public\n",
    "# License along with this library; if not, write to the Free Software\n",
    "# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EuA1RKib8_98"
   },
   "source": [
    "# Mitigating Unwanted Biases with Adversarial Learning\n",
    "\n",
    "Authors: Andrew Zaldivar, Ben Hutchinson, Blake Lemoine, Brian Zhang, Margaret Mitchell\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1x0PRJii9C33"
   },
   "source": [
    "\n",
    "## Summary of this Notebook\n",
    "\n",
    "This notebook is a guide to the paper ([archiv](https://arxiv.org/pdf/1801.07593.pdf))\n",
    "\n",
    "> ```Brian Zhang, Blake Lemoine and Margaret Mitchell. Mitigating Unwanted Biases with Adversarial Learning. AAAI Conference on AI, Ethics and Society, 2018.```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8BnQsDuO9FSt"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "## Intro statement of problem\n",
    "\n",
    "\n",
    "\n",
    "Embeddings are a powerful mechanism for projecting a discrete variable (e.g. words, locales, urls) into a multi-dimensional real valued space.  Several strong methods have been developed for learning embeddings.  One example is the [Skipgram](http://www.cs.brandeis.edu/~marc/misc/proceedings/lrec-2006/pdf/357_pdf.pdf) algorithm.  In that algorithm the surrounding context is used to predict the presence of a word.  Unfortunately, much real world textual data has subtle bias that machine learning algorithms will implicitly include in the embeddings created from that data.  This bias can be illustrated by performing a word analogy task using the learned embeddings.\n",
    "\n",
    "It is worth noting that the usages of terms like _fair_ and _bias_ are used in this notebook in the context of a particular definition of fairness sometimes referred to as \"Demographic Parity\" or \"Equality of Outcomes\" ([Hardt et. al 2016](http://papers.nips.cc/paper/6373-equality-of-opportunity-in-supervised-learning)).  This definition of fairness effectively says that any relationship at all between a variable of interest and a _protected variable_ is an example of unwanted bias.  Other definitions of fairness such as \"Equality of Odds\" can be employed when there is believed to be some form of proper relationship between the variable of interest and the protected variable.  However, all uses of _fair_ and _bias_ here should be interpreted in the context of \"Demographic Parity\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G50xVJvB92z9"
   },
   "source": [
    "First, we'll import all the packages that we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1322
    },
    "colab_type": "code",
    "id": "gOrI3fGc87cz",
    "outputId": "43769798-0a70-4e65-b8a2-2d6cf531d2fd"
   },
   "outputs": [],
   "source": [
    "# !pip install -U gensim~=3.2.0\n",
    "import gensim\n",
    "import gzip\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "# !pip install --upgrade-strategy=only-if-needed tensorflow~=1.6.0rc0\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qrGSp0fA9W8B"
   },
   "source": [
    "Now we'll sync the data for the colab to a tmp directory from Google Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "F_B4Madl_5xp",
    "outputId": "f6bebc8a-9b9f-4d47-ed76-71587e99dd9d"
   },
   "outputs": [],
   "source": [
    "\n",
    "local_dir_name = 'data'\n",
    "\n",
    "WORD2VEC_FILE = os.path.join(local_dir_name+\"/embeddings\", \"GoogleNews-vectors-negative300.bin.gz\")\n",
    "ANALOGIES_FILE = os.path.join(local_dir_name, \"questions-words.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ztVqGJ8xGF1b"
   },
   "outputs": [],
   "source": [
    "def load_word2vec_format(f, max_num_words=None):\n",
    "  \"\"\"Loads word2vec data from a file handle.\n",
    "\n",
    "  Similar to gensim.models.keyedvectors.KeyedVectors.load_word2vec_format\n",
    "  but takes a file handle as input rather than a filename. This lets us use\n",
    "  GFile. Also only accepts binary files.\n",
    "\n",
    "  Args:\n",
    "    f: file handle\n",
    "    max_num_words: number of words to load. If None, load all.\n",
    "\n",
    "  Returns:\n",
    "    Word2vec data as keyedvectors.EuclideanKeyedVectors.\n",
    "  \"\"\"\n",
    "  header = f.readline()\n",
    "  vocab_size, vector_size = (\n",
    "      int(x) for x in header.rstrip().split())  # throws for invalid file format\n",
    "  print \"vector_size =  %d\" % vector_size\n",
    "  result = gensim.models.keyedvectors.EuclideanKeyedVectors()\n",
    "  num_words = 0\n",
    "  result.vector_size = vector_size\n",
    "  result.syn0 = np.zeros((vocab_size, vector_size), dtype=np.float32)\n",
    "  \n",
    "  def add_word(word, weights):\n",
    "    word_id = len(result.vocab)\n",
    "    if word in result.vocab:\n",
    "      print(\"duplicate word '%s', ignoring all but first\", word)\n",
    "      return\n",
    "    result.vocab[word] = gensim.models.keyedvectors.Vocab(\n",
    "        index=word_id, count=vocab_size - word_id)\n",
    "    result.syn0[word_id] = weights\n",
    "    result.index2word.append(word)\n",
    "\n",
    "  if max_num_words and max_num_words < vocab_size:\n",
    "    num_embeddings = max_num_words\n",
    "  else:\n",
    "    num_embeddings = vocab_size\n",
    "  print \"Loading %d embeddings\" % num_embeddings\n",
    "  \n",
    "  binary_len = np.dtype(np.float32).itemsize * vector_size\n",
    "  for _ in xrange(vocab_size):\n",
    "    # mixed text and binary: read text first, then binary\n",
    "    word = []\n",
    "    while True:\n",
    "      ch = f.read(1)\n",
    "      if ch == b' ':\n",
    "        break\n",
    "      if ch == b'':\n",
    "        raise EOFError(\"unexpected end of input; is count incorrect or file otherwise damaged?\")\n",
    "      if ch != b'\\n':  # ignore newlines in front of words (some binary files have)\n",
    "        word.append(ch)\n",
    "    word = gensim.utils.to_unicode(b''.join(word), encoding='utf-8', errors='strict')\n",
    "    weights = np.frombuffer(f.read(binary_len), dtype=np.float32)\n",
    "    add_word(word, weights)\n",
    "    num_words = num_words + 1\n",
    "    if max_num_words and num_words == max_num_words:\n",
    "      break\n",
    "  if result.syn0.shape[0] != len(result.vocab):\n",
    "    print(\n",
    "        \"duplicate words detected, shrinking matrix size from %i to %i\",\n",
    "        result.syn0.shape[0], len(result.vocab))\n",
    "  result.syn0 = np.ascontiguousarray(result.syn0[:len(result.vocab)])\n",
    "  assert (len(result.vocab), vector_size) == result.syn0.shape\n",
    "\n",
    "  print(\"loaded %s matrix\", result.syn0.shape)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "UNAARuwKGHu5",
    "outputId": "58d70bb8-935e-4c9c-e95e-29ffeee47a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word embeddings from data/embeddings/GoogleNews-vectors-negative300.bin.gz\n",
      "vector_size =  300\n",
      "Loading 2000000 embeddings\n",
      "('duplicate words detected, shrinking matrix size from %i to %i', 3000000, 2000000)\n",
      "('loaded %s matrix', (2000000, 300))\n",
      "CPU times: user 47.1 s, sys: 1.5 s, total: 48.6 s\n",
      "Wall time: 48.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Initialize the embeddings client if this hasn't been done yet.\n",
    "# For the efficiency of this notebook we just load the first 2M words, and don't\n",
    "# re-initialize the client if it already exists. You could of course filter the\n",
    "# word list in other ways.\n",
    "if not 'client' in vars():\n",
    "  print \"Loading word embeddings from %s\" % WORD2VEC_FILE\n",
    "  with gzip.GzipFile(fileobj=open(WORD2VEC_FILE, 'r')) as f:\n",
    "    client = load_word2vec_format(f, max_num_words=2000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y_N47nk687UG"
   },
   "source": [
    "The following blocks load a data file with analogy training examples and displays some of them as examples.  By changing the indices selected in the final block you can change which analogies from the training set are being displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pKGlGo5VJnU4"
   },
   "outputs": [],
   "source": [
    "def print_knn(client, v, k):\n",
    "  print \"%d closest neighbors to A-B+C:\" % k\n",
    "  for neighbor, score in client.similar_by_vector(\n",
    "      v.flatten().astype(float), topn=k):\n",
    "    print \"%s : score=%f\" % (neighbor, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eG_tc8kOkMlm"
   },
   "source": [
    "Let's take a look at the analogies that the model generates for *man*:*woman*::*boss*:$\\underline{\\quad}$.\n",
    "Try changing ``\"boss\"`` to ``\"friend\"`` to see further examples of problematic analogies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "6rEfwqtDIt0Q",
    "outputId": "48205f75-b9a3-49ad-8e42-8c41e20be3f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 closest neighbors to A-B+C:\n",
      "smart : score=0.668151\n",
      "intelligent : score=0.454270\n",
      "dumb : score=0.442581\n",
      "smarter : score=0.412079\n",
      "american : score=0.410000\n"
     ]
    }
   ],
   "source": [
    "# Use a word embedding to compute an analogy\n",
    "# Edit the parameters below to get different analogies\n",
    "A = \"asian\"\n",
    "B = \"american\"\n",
    "C = \"smart\"\n",
    "NUM_ANALOGIES = 5\n",
    "\n",
    "in_arr = []\n",
    "for i, word in enumerate((A, B, C)):\n",
    "  in_arr.append(client.word_vec(word))\n",
    "in_arr = np.array([in_arr])\n",
    "\n",
    "print_knn(client, -in_arr[0, 0, :] + in_arr[0, 1, :] + in_arr[0, 2, :],\n",
    "          NUM_ANALOGIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CLhtOKQKKN4W"
   },
   "outputs": [],
   "source": [
    "def load_analogies(filename):\n",
    "  \"\"\"Loads analogies.\n",
    "\n",
    "  Args:\n",
    "    filename: the file containing the analogies.\n",
    "\n",
    "  Returns:\n",
    "    A list containing the analogies.\n",
    "  \"\"\"\n",
    "  analogies = []\n",
    "  with open(filename, \"r\") as fast_file:\n",
    "    for line in fast_file:\n",
    "      line = line.strip()\n",
    "      # in the analogy file, comments start with :\n",
    "      if line[0] == \":\":\n",
    "        continue\n",
    "      words = line.split()\n",
    "      # there are no misformatted lines in the analogy file, so this should\n",
    "      # only happen once we're done reading all analogies.\n",
    "      if len(words) != 4:\n",
    "        print \"Invalid line: %s\" % line\n",
    "        continue\n",
    "      analogies.append(words)\n",
    "  print \"loaded %d analogies\" % len(analogies)\n",
    "  return analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "2RtfKMshE0gy",
    "outputId": "c806fe01-7bc4-4252-d7d2-3edc2b73f336"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 19544 analogies\n",
      "write is to writes as increase is to increases\n",
      "write is to writes as listen is to listens\n",
      "write is to writes as play is to plays\n",
      "write is to writes as predict is to predicts\n",
      "write is to writes as provide is to provides\n",
      "write is to writes as say is to says\n",
      "write is to writes as scream is to screams\n",
      "write is to writes as search is to searches\n",
      "write is to writes as see is to sees\n",
      "write is to writes as shuffle is to shuffles\n",
      "write is to writes as sing is to sings\n",
      "write is to writes as sit is to sits\n",
      "write is to writes as slow is to slows\n",
      "write is to writes as speak is to speaks\n",
      "write is to writes as swim is to swims\n",
      "write is to writes as talk is to talks\n",
      "write is to writes as think is to thinks\n",
      "write is to writes as vanish is to vanishes\n",
      "write is to writes as walk is to walks\n",
      "write is to writes as work is to works\n"
     ]
    }
   ],
   "source": [
    "analogies = load_analogies(ANALOGIES_FILE)\n",
    "print \"\\n\".join(\"%s is to %s as %s is to %s\" % tuple(x) for x in analogies[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w4LQ8JcJKXmU"
   },
   "source": [
    "## Adversarial Networks for Bias Mitigation\n",
    "\n",
    "The method presented here for removing some of the bias from embeddings is based on the idea that those embeddings are intended to be used to predict some outcome $Y$ based on an input $X$ but that outcome should, in a fair world, be completely unrelated to some protected variable $Z$.  If that were the case then knowing $Y$ would not help you predict $Z$ any better than chance.  This principle can be directly translated into two networks in series as illustrated below.  The first attempts to predict $Y$ using $X$ as input.  The second attempts to use the predicted value of $Y$ to predict $Z$.  See Figure 1 of [the paper](https://arxiv.org/pdf/1801.07593.pdf).\n",
    "\n",
    "However, simply training the weights in W based on $\\nabla_WL_1$ and the weights in $U$ based on $\\nabla_UL_2$ won’t actually achieve an unbiased model.  In order to do that you need to incorporate into $W$’s update function the concept that $U$ should be no better than chance at predicting $Z$.  The way that you can achieve that is analogous to how Generative Adversarial Networks (GANs) ([Goodfellow et al. 2014](http://papers.nips.cc/paper/5423-generative-adversarial-nets)) train their generators.\n",
    "\n",
    "In addition to $\\nabla_WL_1$ you incorporate the negation of $\\nabla_WL_2$ into $W$’s update function.  However, it’s possible that $\\nabla_WL_1$ is changing $W$ in a way which will improve accuracy by using the biased information you are trying to protect.  In order to avoid that you also incorporate a term which removes that component of $\\nabla_WL_1$ by projecting it onto $\\nabla_WL_2$.  Once you’ve incorporated those two terms, the update function for $W$ becomes:\n",
    "\n",
    "\n",
    "$\\nabla_WL_1-proj_{(\\nabla_WL_2)}\\nabla_WL_1 - \\nabla_WL_2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kwfqHFzAKgiV"
   },
   "source": [
    "### Defining the Protected Variable of Embeddings\n",
    "\n",
    "The description of how to incorporate adversarial networks into machine learned models above is very generic because the technique is generally applicable for any type of systems which can be described in terms of input $X$ being predictive of $Y$ but potentially containing information about a protected variable $Z$.  So long as you can construct the relevant update functions you can apply this technique.  However, that doesn’t tell you much about the nature of $X$, $Y$ and $Z$.  In the case of the word analogies task above, $X = B + C - A$ and $Y = D$.  Figuring out what $Z$ should be is a little bit trickier though.  For that we can look to a paper by [Bulokbasi et. al.](http://papers.nips.cc/paper/6227-man-is-to-computer-programmer-as-woman-is-to-homemaker-debiasing-word-embeddings) where they developed an unsupervised methodology for removing gendered semantics from word embeddings.\n",
    "\n",
    "The first step is to select pairs of words which are relevant to the type of bias you are trying to remove.  In the case of gender you can choose word pairs like “man”:”woman” and “boy”:girl” which have gender as the only difference in their semantics.  Once you have these word pairs you can compute the difference between their embeddings to produce vectors in the embeddings’ semantic space which are roughly parallel to the semantics of gender.  Performing Principal Components Analysis (PCA) on those vectors then gives you the major components of the semantics of gender as defined by the gendered word pairs provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pM8NTb7bKo_5"
   },
   "outputs": [],
   "source": [
    "def _np_normalize(v):\n",
    "  \"\"\"Returns the input vector, normalized.\"\"\"\n",
    "  return v / np.linalg.norm(v)\n",
    "\n",
    "\n",
    "def load_vectors(client, analogies):\n",
    "  \"\"\"Loads and returns analogies and embeddings.\n",
    "\n",
    "  Args:\n",
    "    client: the client to query.\n",
    "    analogies: a list of analogies.\n",
    "\n",
    "  Returns:\n",
    "    A tuple with:\n",
    "    - the embedding matrix itself\n",
    "    - a dictionary mapping from strings to their corresponding indices\n",
    "      in the embedding matrix\n",
    "    - the list of words, in the order they are found in the embedding matrix\n",
    "  \"\"\"\n",
    "  words_unfiltered = set()\n",
    "  for analogy in analogies:\n",
    "    words_unfiltered.update(analogy)\n",
    "  print \"found %d unique words\" % len(words_unfiltered)\n",
    "\n",
    "  vecs = []\n",
    "  words = []\n",
    "  index_map = {}\n",
    "  for word in words_unfiltered:\n",
    "    try:\n",
    "      vecs.append(_np_normalize(client.word_vec(word)))\n",
    "      index_map[word] = len(words)\n",
    "      words.append(word)\n",
    "    except KeyError:\n",
    "      print \"word not found: %s\" % word\n",
    "  print \"words not filtered out: %d\" % len(words)\n",
    "\n",
    "  return np.array(vecs), index_map, words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "6TLg0wygKKW0",
    "outputId": "95102bb6-7227-4ffe-ca02-792d3981ecdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 905 unique words\n",
      "words not filtered out: 905\n",
      "word embedding dimension: 300\n"
     ]
    }
   ],
   "source": [
    "embed, indices, words = load_vectors(client, analogies)\n",
    "\n",
    "embed_dim = len(embed[0].flatten())\n",
    "print \"word embedding dimension: %d\" % embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SFSGOaqDLPij"
   },
   "outputs": [],
   "source": [
    "def find_gender_direction(embed,\n",
    "                          indices):\n",
    "  \"\"\"Finds and returns a 'gender direction'.\"\"\"\n",
    "  pairs = [\n",
    "      (\"woman\", \"man\"),\n",
    "      (\"her\", \"his\"),\n",
    "      (\"she\", \"he\"),\n",
    "      (\"aunt\", \"uncle\"),\n",
    "      (\"niece\", \"nephew\"),\n",
    "      (\"daughters\", \"sons\"),\n",
    "      (\"mother\", \"father\"),\n",
    "      (\"daughter\", \"son\"),\n",
    "      (\"granddaughter\", \"grandson\"),\n",
    "      (\"girl\", \"boy\"),\n",
    "      (\"stepdaughter\", \"stepson\"),\n",
    "      (\"mom\", \"dad\"),\n",
    "  ]\n",
    "  m = []\n",
    "  for wf, wm in pairs:\n",
    "    m.append(embed[indices[wf]] - embed[indices[wm]])\n",
    "  m = np.array(m)\n",
    "\n",
    "  # the next three lines are just a PCA.\n",
    "  m = np.cov(np.array(m).T)\n",
    "  evals, evecs = np.linalg.eig(m)\n",
    "  return _np_normalize(np.real(evecs[:, np.argmax(evals)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1320
    },
    "colab_type": "code",
    "id": "fSj8daFnKvNr",
    "outputId": "c9de79bb-13ea-49fd-95b5-933dad3f286a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender direction: [-7.94102556e-02 -8.62907447e-02 -9.36046086e-02 -6.82094699e-02\n",
      "  2.10405600e-02  9.09854549e-03  3.69967887e-02  6.76862493e-03\n",
      " -1.35619514e-01  3.62402266e-02  6.62573091e-02 -9.91207519e-04\n",
      "  4.59588196e-02 -6.13280986e-02 -2.14388384e-03 -1.15234663e-02\n",
      " -7.85432247e-02 -6.24935300e-02 -8.79221813e-02 -1.73189498e-02\n",
      "  1.72767315e-02  1.72292006e-03 -3.53863093e-02  5.24758140e-03\n",
      "  1.45531278e-02  7.97479425e-03 -3.40466363e-02 -2.83068019e-02\n",
      " -1.02746018e-01 -1.78703381e-01 -1.88934471e-02 -7.23408473e-02\n",
      " -1.06853722e-01 -4.48557103e-02 -5.89368281e-03 -7.94341237e-02\n",
      "  1.17833895e-03 -5.86551743e-02  3.10732848e-02 -3.12044830e-03\n",
      " -4.15907969e-02 -7.17691663e-03 -2.56242105e-02 -3.41547309e-02\n",
      " -6.38683437e-02  7.83023380e-02  1.41320146e-03 -1.92862106e-02\n",
      "  4.27428205e-02  6.23987501e-03  2.58495545e-02 -2.53252181e-02\n",
      " -6.58270803e-03  6.81094699e-02  2.29536006e-02 -3.01465541e-02\n",
      " -8.76620455e-03 -3.48142318e-03 -1.03283556e-02 -4.43229749e-02\n",
      "  4.82072717e-02 -1.38947109e-01 -5.49295845e-02  1.42456083e-01\n",
      " -3.82443506e-02  5.43604495e-02  6.42165093e-02  1.80136131e-01\n",
      " -2.95630757e-02 -7.22149244e-02  1.53964050e-02  3.28869800e-02\n",
      "  4.24435462e-02  2.30966085e-02  1.12612003e-02  4.36423822e-02\n",
      " -7.29707357e-04 -3.64472450e-02  7.50969536e-02  7.15724467e-02\n",
      " -2.49016159e-02 -3.88707157e-02  2.47988546e-02  8.84891062e-03\n",
      " -1.57366197e-01  5.67419234e-02 -5.13816362e-02  1.39329337e-02\n",
      " -2.49230367e-03 -8.79131791e-03 -6.95209856e-02 -9.16509235e-02\n",
      " -5.41995965e-02  1.73404339e-02 -6.23635240e-03  1.97633141e-02\n",
      " -1.39783648e-02 -2.09286823e-02  5.42969581e-02  5.43749286e-02\n",
      " -2.15415313e-02 -2.41145593e-03  4.85206319e-02 -5.10697951e-02\n",
      " -1.34003908e-02  2.13284548e-02 -1.12171602e-01 -3.89264077e-02\n",
      "  7.07023362e-02  6.22944837e-03 -2.23500522e-02 -5.02853247e-02\n",
      "  9.33401918e-02  2.40351194e-02 -4.21102256e-02  5.20160292e-02\n",
      "  4.75287371e-02 -8.11995185e-02 -7.38389538e-03  2.29741450e-02\n",
      "  7.62025009e-03  5.25651699e-02  1.24644176e-01 -3.22192007e-02\n",
      "  9.16384819e-02  1.89994690e-01  7.22935579e-03  5.25197973e-02\n",
      "  8.30303086e-02  6.13005472e-02  4.64472574e-02  1.97908008e-02\n",
      " -5.19907059e-02  1.51321854e-01  2.47586642e-02  7.32033143e-02\n",
      "  2.25787357e-02 -5.42248469e-02 -6.66883855e-02 -1.05413345e-01\n",
      " -2.27129787e-02 -8.88976863e-02 -3.63838620e-02 -8.98777237e-02\n",
      "  5.52357322e-03  3.68328109e-02 -6.02720613e-02  7.25324488e-02\n",
      " -6.96779923e-03 -1.40505525e-01  2.02638063e-03 -3.35120036e-04\n",
      "  7.33190982e-02 -6.88252971e-02  4.61441135e-02  7.68183901e-02\n",
      " -3.30161505e-02  8.79513077e-02  7.73010673e-02  9.25715579e-03\n",
      " -9.04122538e-02  7.52339842e-03 -5.72907134e-02  4.28152602e-02\n",
      "  2.86397606e-02 -5.62480396e-02 -3.87299617e-02 -3.87058619e-02\n",
      "  5.85371538e-02  9.43237537e-02  2.71958769e-02  5.25680701e-02\n",
      "  2.08041380e-02 -3.83978501e-02 -1.10333866e-01 -1.88310924e-02\n",
      "  1.85601924e-02  1.11566914e-01 -3.58018442e-04  2.06100921e-02\n",
      "  8.06141199e-03 -1.22897665e-02 -2.16353311e-02  8.96372961e-03\n",
      "  1.00744761e-02 -2.87074612e-02  1.11815260e-02  5.87513547e-02\n",
      " -5.30042040e-02  1.27520561e-01  1.25319079e-02 -1.03226821e-01\n",
      " -1.97484407e-02 -3.42320863e-02  1.05197274e-01  1.76135002e-02\n",
      "  2.80467006e-02 -8.31193905e-03  1.10601081e-02 -1.40255927e-02\n",
      " -4.74642660e-02 -2.75716115e-03 -1.01467816e-01 -5.44390020e-02\n",
      " -8.95335277e-02 -1.65279682e-02 -1.12849845e-01  3.43099446e-02\n",
      "  1.07627595e-02 -1.95155440e-02  8.49218426e-03  7.18273491e-02\n",
      "  9.43337430e-02  5.93970694e-02  4.42150488e-02  3.93104455e-03\n",
      "  1.55634159e-02  1.47405106e-02  9.86230904e-02 -3.51293172e-02\n",
      " -5.48265317e-03  4.39100322e-02  1.00173280e-01  4.73382845e-02\n",
      " -8.75985652e-02 -8.01811478e-02 -2.84562342e-02 -1.62217727e-02\n",
      " -5.23065164e-02  2.49845262e-02 -3.01654996e-02  1.45479843e-02\n",
      " -1.35092122e-02 -1.05868228e-02  1.30148387e-04  1.41828245e-03\n",
      "  9.64396967e-02 -1.37944939e-03  1.28795558e-02  5.14631073e-02\n",
      "  4.12590524e-02 -1.05768228e-01  4.51128868e-02  7.65950631e-02\n",
      "  8.35771929e-02  5.04364947e-02 -7.37407299e-02 -1.27911956e-02\n",
      " -7.96467866e-02  1.42361577e-02  2.28048101e-02 -2.45567372e-02\n",
      "  1.51617365e-02 -8.14192396e-02 -2.99888041e-02 -9.38109909e-03\n",
      "  9.73554448e-03  2.77240676e-02 -1.44856280e-02 -2.15535602e-02\n",
      "  2.22796961e-02 -2.44437577e-02 -1.73757435e-04 -5.92882308e-02\n",
      " -6.75396381e-02 -6.88861185e-03  1.79634043e-02  5.94912151e-02\n",
      "  1.34743549e-02  3.44577542e-02 -5.31342611e-02  1.22593336e-02\n",
      "  6.63454886e-03  1.72750748e-01  8.17975838e-02  7.98060175e-03\n",
      "  3.48464502e-02  3.15494960e-02  3.00879598e-02  1.26706401e-02\n",
      " -5.09294800e-02 -1.49508612e-02  5.45480741e-03  5.96511080e-02\n",
      "  8.97118066e-03  2.17049865e-02 -5.54851264e-02  4.33567631e-03\n",
      "  4.53374907e-02  9.27058295e-02 -2.28239338e-02  3.36064502e-02\n",
      " -5.63768244e-02 -4.46595961e-02  2.17583523e-03 -3.09787628e-02\n",
      " -1.12975804e-02  9.51682266e-02  3.74056898e-02  1.14501412e-01]\n"
     ]
    }
   ],
   "source": [
    "# Using the embeddings, find the gender vector.\n",
    "gender_direction = find_gender_direction(embed, indices)\n",
    "print \"gender direction: %s\" % str(gender_direction.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pVJIOmh2LcV7"
   },
   "source": [
    "Once you have the first principal component of the embedding differences, you can start projecting the embeddings of words onto it.  That projection is roughly the degree to which a word is relevant to the latent protected variable defined by the first principle component of the word pairs given.  This projection can then be taken as the protected variable $Z$ which the adversary is attempting to predict on the basis of the predicted value of $Y$.  The code below illustrates how to construct a function which computes $Z$ from $X$ in this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FpOpO6RGBznY"
   },
   "source": [
    "Try editing the WORD param in the next cell to see the projection of other words onto the gender direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexicon(filename):\n",
    "    lexicon = []\n",
    "    with open(filename) as infile:\n",
    "        for line in infile:\n",
    "            line = line.rstrip()\n",
    "            if line and not line.startswith(';'):\n",
    "                lexicon.append(line)\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words = load_lexicon(\"data/opinion_lexicon/positive-words.txt\")\n",
    "neg_words = load_lexicon(\"data/opinion_lexicon/negative-words.txt\")\n",
    "pos_words_fil = filter(lambda x:  x in embeddings,map(lambda x: x,pos_words))\n",
    "neg_words_fil = filter(lambda x:  x in embeddings,map(lambda x: x,neg_words))\n",
    "pos_vectors = embeddings[pos_words_fil]\n",
    "neg_vectors = embeddings[neg_words_fil]\n",
    "vectors = np.concatenate([pos_vectors, neg_vectors])\n",
    "targets = np.array([0 for entry in pos_vectors] + [1 for entry in neg_vectors])\n",
    "train_vectors, test_vectors, train_targets, test_targets = train_test_split(vectors, targets, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.946031746031746"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs',\n",
    "                         multi_class='auto').fit(train_vectors,train_targets)\n",
    "clf.score(test_vectors,test_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 100 artists>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import wordnet \n",
    "good = []\n",
    "bad=  []\n",
    "\n",
    "for word in pos_words:\n",
    "    a = wordnet.synsets(word)\n",
    "    if len(a)>0:\n",
    "        syn = a[0] .lemmas()[0]\n",
    "        if len(syn.antonyms())>0:\n",
    "            good.append(word)\n",
    "            bad.append(syn.antonyms()[0].name()) \n",
    "            \n",
    "good_vectors=[]\n",
    "bad_vectors = []\n",
    "# good = ['good','beautiful','happy','positive','compentent','smart','interesting','pleasant','best','honest','wealthy','wonderful']\n",
    "# bad = ['bad','ugly','sad','negative','incompetent','dumb','boring','horrible','worst','corrupt','poor','awful']\n",
    "# good = ['amazing']\n",
    "# bad = ['horrible']\n",
    "choices = np.random.choice(len(good),20)\n",
    "for i,j in zip(good,bad):\n",
    "    if i in embeddings and j in embeddings:\n",
    "        good_vectors.append(_np_normalize(embeddings[i]))\n",
    "        bad_vectors.append(_np_normalize(embeddings[j]))\n",
    "\n",
    "good_vectors = np.array(good_vectors)\n",
    "bad_vectors = np.array(bad_vectors)\n",
    "X = good_vectors-bad_vectors\n",
    "m = np.dot(X.T,X)\n",
    "evals, evecs = np.linalg.eig(m)\n",
    "vec = _np_normalize(np.real(evecs[:, np.argmax(evals)]))\n",
    "plt.bar(np.arange(100),evals[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BipW3yjvLJFz",
    "outputId": "3f3c0ebe-2733-463e-9a6b-4e9d6a62b0b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06925793\n"
     ]
    }
   ],
   "source": [
    "WORD = \"polish\"\n",
    "word_vec = _np_normalize(client.word_vec(WORD))\n",
    "print word_vec.dot(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "identities = ['lesbian', 'gay', 'bisexual', 'transgender', 'trans', 'queer', \n",
    "              'lgbt', 'lgbtq', 'homosexual', 'straight', 'heterosexual', 'male', \n",
    "              'female', 'nonbinary', 'african', 'african american', 'black', 'white', \n",
    "              'european', 'hispanic', 'latino', 'latina', 'latinx', 'mexican', 'canadian', \n",
    "              'american', 'asian', 'indian', 'middle eastern', 'chinese', 'japanese', \n",
    "              'christian', 'muslim', 'jewish', 'buddhist', 'catholic', 'protestant', 'sikh', \n",
    "              'taoist', 'old', 'older', 'young', 'younger', 'teenage', 'millenial', 'middle aged', \n",
    "              'elderly', 'blind', 'deaf', 'paralyzed']\n",
    "nationalities=[\n",
    "'German',\n",
    "#'African_American',\n",
    "'Mexican',\n",
    "'Irish',\n",
    "'English',\n",
    "'American',\n",
    "'Italian',\n",
    "'Polish',\n",
    "'French',\n",
    "'Scottish',\n",
    "#'Puerto_Rican',\n",
    "'Norwegian',\n",
    "'Dutch',\n",
    "'Swedish',\n",
    "'Chinese',\n",
    "'Indian',\n",
    "'Russian',\n",
    "'Filipino'\n",
    "]\n",
    "religions =[ \n",
    "    'Zionist',\n",
    "    'Catholic',\n",
    "    'Christian',\n",
    "    'Islamic',\n",
    "    'Protestant',\n",
    "    'Taoist',\n",
    "    'Atheist',\n",
    "    'Hindu',\n",
    "    'Buddhist',\n",
    "    'Diasporic',\n",
    "    'Sikh',\n",
    "    'Juche',\n",
    "    'Jewish',\n",
    "    'Bahai',\n",
    "    'Jains',\n",
    "    'Shinto',\n",
    "    'Pagan'\n",
    "]\n",
    "gender= [\"male\",\"female\",\"her\",\"him\",\"man\",\"woman\",\"boy\",\"girl\",\"his\",\"hers\",\"mom\",\"dad\"]#perhaps try all terms intraprotected group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      word  sentiment_score\n",
      "0     male        -0.222274\n",
      "6      boy        -0.204027\n",
      "7     girl        -0.202659\n",
      "5    woman        -0.156328\n",
      "4      man        -0.127307\n",
      "9     hers        -0.111194\n",
      "3      him        -0.091149\n",
      "8      his        -0.068180\n",
      "2      her        -0.054850\n",
      "1   female        -0.044018\n",
      "10     mom         0.029774\n",
      "11     dad         0.072020\n"
     ]
    }
   ],
   "source": [
    "words = gender#filter(lambda x: x.lower() in embeddings,identities)\n",
    "df = pd.DataFrame(data={\"word\": list(words)})\n",
    "df[\"sentiment_score\"] = df[\"word\"].map(\n",
    "    lambda w: embeddings[w.lower()].dot(vec))\n",
    "df.sort_values(by=\"sentiment_score\", inplace=True)\n",
    "print df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RlxOV3RZBpou"
   },
   "source": [
    "Let's now look at the words with the largest *negative* projection onto the gender dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "rxjWqgCXLe6S",
    "outputId": "c50559f6-d01a-45f2-a4ef-05859092e6bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              word  sentiment_score\n",
      "304   unproductive        -1.271479\n",
      "217     irrational        -1.159424\n",
      "45   uncompetitive        -1.143645\n",
      "380    inefficient        -1.130281\n",
      "428     uninformed        -1.116056\n",
      "579  irresponsible        -1.095759\n",
      "83       illogical        -1.091005\n",
      "743      dishonest        -1.057061\n",
      "21   uninformative        -1.034158\n",
      "503   unacceptable        -1.017415\n"
     ]
    }
   ],
   "source": [
    "words = set()\n",
    "for a in analogies:\n",
    "  words.update(a)\n",
    "\n",
    "df = pd.DataFrame(data={\"word\": list(words)})\n",
    "df[\"sentiment_score\"] = df[\"word\"].map(\n",
    "    lambda w: client.word_vec(w).dot(vec))\n",
    "df.sort_values(by=\"sentiment_score\", inplace=True)\n",
    "print df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v9fx985BByiU"
   },
   "source": [
    "Let's now look at the words with the largest *positive* projection onto the gender dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "id": "JTLLr12vB2mN",
    "outputId": "a9174598-802a-496e-ddf5-8b41483fb2c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            word  sentiment_score\n",
      "711       widest         0.962137\n",
      "425    efficient         0.860398\n",
      "370     sweetest         0.804517\n",
      "283    fortunate         0.728857\n",
      "14      tastiest         0.724279\n",
      "415  comfortable         0.714410\n",
      "115        great         0.703595\n",
      "568  efficiently         0.703564\n",
      "51   informative         0.689995\n",
      "829    enhancing         0.677500\n"
     ]
    }
   ],
   "source": [
    "df.sort_values(by=\"sentiment_score\", inplace=True, ascending=False)\n",
    "print df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rMEGkmSlTsR6"
   },
   "source": [
    "### Training the model\n",
    "\n",
    "Training adversarial networks is hard. They are touchy, and if touched the wrong way, they blow up VERY quickly. One must be very careful to train both models slowly enough, so that the parameters in the models do not diverge. In practice, this usually entails significantly lowering the step size of both the classifier and the adversary. It is also probably beneficial to initialize the parameters of the adversary to be extremely small, to ensure that the classifier does not overfit against a particular (sub-optimal) adversary (such overfitting can very quickly cause divergence!).  It is also possible that if the classifier is too good at hiding the protected variable from the adversary then the adversary will impose updates that diverge in an effort to improve its performance.  The solution to that can sometimes be to actually increase the adversary’s learning rate to prevent divergence (something almost unheard of in most learning systems).  Below is a system for learning the debiasing model for word embeddings described above.  \n",
    "\n",
    "Inspect the terminal output for the kernel to inspect the performance of the model. Try modifying the hyperparameters at the top to see how that impacts the convergence properties of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7tsGeQLKT3ZY"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "def tf_normalize(x):\n",
    "  \"\"\"Returns the input vector, normalized.\n",
    "\n",
    "  A small number is added to the norm so that this function does not break when\n",
    "  dealing with the zero vector (e.g. if the weights are zero-initialized).\n",
    "\n",
    "  Args:\n",
    "    x: the tensor to normalize\n",
    "  \"\"\"\n",
    "  return x / (tf.norm(x) + np.finfo(np.float32).tiny)\n",
    "\n",
    "\n",
    "class AdversarialEmbeddingModel(object):\n",
    "  \"\"\"A model for doing adversarial training of embedding models.\"\"\"\n",
    "\n",
    "  def __init__(self, client,\n",
    "               data_p, embed_dim, projection,\n",
    "               projection_dims, pred,X,Y,X_test,Y_test):\n",
    "    \"\"\"Creates a new AdversarialEmbeddingModel.\n",
    "\n",
    "    Args:\n",
    "      client: The (possibly biased) embeddings.\n",
    "      data_p: Placeholder for the data.\n",
    "      embed_dim: Number of dimensions used in the embeddings.\n",
    "      projection: The space onto which we are \"projecting\".\n",
    "      projection_dims: Number of dimensions of the projection.\n",
    "      pred: Prediction layer.\n",
    "    \"\"\"\n",
    "    # load the analogy vectors as well as the embeddings\n",
    "    self.client = client\n",
    "    self.data_p = data_p\n",
    "    self.embed_dim = embed_dim\n",
    "    self.projection = projection\n",
    "    self.projection_dims = projection_dims\n",
    "    self.pred = pred\n",
    "    self.X=X\n",
    "    self.Y=Y\n",
    "    self.X_test = X_test\n",
    "    self.Y_test = Y_test\n",
    "\n",
    "  def nearest_neighbors(self, sess, in_arr,\n",
    "                        k):\n",
    "    \"\"\"Finds the nearest neighbors to a vector.\n",
    "\n",
    "    Args:\n",
    "      sess: Session to use.\n",
    "      in_arr: Vector to find nearest neighbors to.\n",
    "      k: Number of nearest neighbors to return\n",
    "    Returns:\n",
    "      List of up to k pairs of (word, score).\n",
    "    \"\"\"\n",
    "    v = sess.run(self.pred, feed_dict={self.data_p: in_arr})\n",
    "    return self.client.similar_by_vector(v.flatten().astype(float), topn=k)\n",
    "\n",
    "  def write_to_file(self, sess, f):\n",
    "    \"\"\"Writes a model to disk.\"\"\"\n",
    "    np.savetxt(f, sess.run(self.projection))\n",
    "\n",
    "  def read_from_file(self, sess, f):\n",
    "    \"\"\"Reads a model from disk.\"\"\"\n",
    "    loaded_projection = np.loadtxt(f).reshape(\n",
    "        [self.embed_dim, self.projection_dims])\n",
    "    sess.run(self.projection.assign(loaded_projection))\n",
    "    \n",
    "  def fit(self,\n",
    "          sess,\n",
    "          data,\n",
    "          data_p,\n",
    "          labels,\n",
    "          labels_p,\n",
    "          protect,\n",
    "          protect_p,\n",
    "          gender_direction,\n",
    "          pred_learning_rate,\n",
    "          protect_learning_rate,\n",
    "          protect_loss_weight,\n",
    "          num_steps,\n",
    "          batch_size,\n",
    "          debug_interval=1000):\n",
    "    \"\"\"Trains a model.\n",
    "\n",
    "    Args:\n",
    "      sess: Session.\n",
    "      data: Features for the training data.\n",
    "      data_p: Placeholder for the features for the training data.\n",
    "      labels: Labels for the training data.\n",
    "      labels_p: Placeholder for the labels for the training data.\n",
    "      protect: Protected variables.\n",
    "      protect_p: Placeholder for the protected variables.\n",
    "      gender_direction: The vector from find_gender_direction().\n",
    "      pred_learning_rate: Learning rate for predicting labels.\n",
    "      protect_learning_rate: Learning rate for protecting variables.\n",
    "      protect_loss_weight: The constant 'alpha' found in\n",
    "          debias_word_embeddings.ipynb.\n",
    "      num_steps: Number of training steps.\n",
    "      batch_size: Number of training examples in each step.\n",
    "      debug_interval: Frequency at which to log performance metrics during\n",
    "          training.\n",
    "    \"\"\"\n",
    "    #################################################################\n",
    "    learning_rate = 0.001\n",
    "    training_epochs = 100\n",
    "    batch_size = 100\n",
    "    display_step = 1\n",
    "\n",
    "\n",
    "    # Network Parameters\n",
    "    n_hidden_1 = 10 # 1st layer number of features\n",
    "    n_hidden_2 = 10 # 2nd layer number of features\n",
    "    n_input = 300 # Number of feature\n",
    "    n_classes = 2 # Number of classes to predict\n",
    "\n",
    "\n",
    "    # tf Graph input\n",
    "    x = tf.placeholder(\"float\", [None, n_input])\n",
    "    y = tf.placeholder(\"float\", [None, n_classes])\n",
    "    y_targets = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "    # Create model\n",
    "    def multilayer_perceptron(x, weights, biases):\n",
    "        # Hidden layer with RELU activation\n",
    "        layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "        layer_1 = tf.nn.relu(layer_1)\n",
    "        # Hidden layer with RELU activation\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "        layer_2 = tf.nn.relu(layer_2)\n",
    "        # Output layer with linear activation\n",
    "        out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "        return out_layer\n",
    "\n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "    }\n",
    "\n",
    "    # Construct model\n",
    "    pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "    ##########################\n",
    "    feed_dict = {\n",
    "        data_p: data,\n",
    "        labels_p: labels,\n",
    "        protect_p: protect,\n",
    "    }\n",
    "    # define the prediction loss\n",
    "    pred_loss = tf.losses.mean_squared_error(labels_p, self.pred)\n",
    "\n",
    "    # compute the prediction of the protected variable.\n",
    "    # The \"trainable\"/\"not trainable\" designations are for the predictor. The\n",
    "    # adversary explicitly specifies its own list of weights to train.\n",
    "#     protect_weights = tf.get_variable(\n",
    "#         \"protect_weights\", [self.embed_dim, 1], trainable=False)\n",
    "    \n",
    "#     protect_pred = tf.matmul(self.pred, protect_weights)#changr\n",
    "#     protect_loss = tf.losses.mean_squared_error(protect_p, protect_pred)\n",
    "\n",
    "    pred_cr = multilayer_perceptron(self.pred, weights, biases)\n",
    "    protect_loss =  tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred_cr, labels=y_targets))\n",
    "    print protect_loss\n",
    "\n",
    "    pred_opt = tf.train.AdamOptimizer(pred_learning_rate)\n",
    "#     protect_opt = tf.train.AdamOptimizer(protect_learning_rate)\n",
    "\n",
    "    protect_grad = {v: g for (g, v) in pred_opt.compute_gradients(protect_loss)}\n",
    "    pred_grad = []\n",
    "\n",
    "    # applies the gradient expression found in the document linked\n",
    "    # at the top of this file.\n",
    "    for (g, v) in pred_opt.compute_gradients(pred_loss):\n",
    "      unit_protect = tf_normalize(protect_grad[v])\n",
    "      # the two lines below can be commented out to train without debiasing\n",
    "      g -= tf.reduce_sum(g * unit_protect) * unit_protect\n",
    "      g -= protect_loss_weight * protect_grad[v]\n",
    "      pred_grad.append((g, v))\n",
    "      pred_min = pred_opt.apply_gradients(pred_grad)\n",
    "\n",
    "    # compute the loss of the protected variable prediction.\n",
    "#     protect_min = protect_opt.minimize(protect_loss, var_list=[protect_weights])\n",
    "  # Parameters\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    #####################################################################\n",
    "    \n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(len(self.X)/batch_size)\n",
    "        X_batches = np.array_split(self.X, total_batch)\n",
    "        Y_batches = np.array_split(self.Y, total_batch)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = X_batches[i], Y_batches[i]\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            print np.array([[.5,.5]]*len(batch_y)), batch_y\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y,\n",
    "                                                         y_targets:np.array([[.5,.5]]*len(batch_y))\n",
    " })\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: self.X_test, y: self.Y_test}))\n",
    "    global result \n",
    "    result = tf.argmax(pred, 1).eval({x: self.X_test, y: self.Y_test})\n",
    "    ###############################################################################\n",
    "    step = 0\n",
    "    while step < num_steps:\n",
    "      # pick samples at random without replacement as a minibatch\n",
    "      ids = np.random.choice(len(data), batch_size, False)\n",
    "      data_s, labels_s, protect_s = data[ids], labels[ids], protect[ids]\n",
    "      sgd_feed_dict = {\n",
    "          data_p: data_s,\n",
    "          labels_p: labels_s,\n",
    "          protect_p: protect_s,\n",
    "      }\n",
    "\n",
    "      if not step % debug_interval:\n",
    "        metrics = [pred_loss, protect_loss, self.projection]\n",
    "        metrics_o = sess.run(metrics, feed_dict=feed_dict)\n",
    "        pred_loss_o, protect_loss_o, proj_o = metrics_o\n",
    "        # log stats every so often: number of steps that have passed,\n",
    "        # prediction loss, adversary loss\n",
    "        print(\"step: %d; pred_loss_o: %f; protect_loss_o: %f\" % (step,\n",
    "                     pred_loss_o, protect_loss_o))\n",
    "        for i in range(proj_o.shape[1]):\n",
    "          print(\"proj_o: %f; dot(proj_o, gender_direction): %f)\" %\n",
    "                       (np.linalg.norm(proj_o[:, i]),\n",
    "                       np.dot(proj_o[:, i].flatten(), gender_direction)))\n",
    "      sess.run([pred_min, protect_min], feed_dict=sgd_feed_dict)\n",
    "      step += 1\n",
    "      \n",
    "def filter_analogies(analogies,\n",
    "                     index_map):\n",
    "  filtered_analogies = []\n",
    "  for analogy in analogies:\n",
    "    if filter(index_map.has_key, analogy) != analogy:\n",
    "      print \"at least one word missing for analogy: %s\" % analogy\n",
    "    else:\n",
    "      filtered_analogies.append(map(index_map.get, analogy))\n",
    "  return filtered_analogies\n",
    "\n",
    "def make_data(\n",
    "    analogies, embed,\n",
    "    gender_direction):\n",
    "  \"\"\"Preps the training data.\n",
    "\n",
    "  Args:\n",
    "    analogies: a list of analogies\n",
    "    embed: the embedding matrix from load_vectors\n",
    "    gender_direction: the gender direction from find_gender_direction\n",
    "\n",
    "  Returns:\n",
    "    Three numpy arrays corresponding respectively to the input, output, and\n",
    "    protected variables.\n",
    "  \"\"\"\n",
    "  data = []\n",
    "  labels = []\n",
    "  protect = []\n",
    "  for analogy in analogies:\n",
    "    # the input is just the word embeddings of the first three words\n",
    "    data.append(embed[analogy[:3]])\n",
    "    # the output is just the word embeddings of the last word\n",
    "    labels.append(embed[analogy[3]])\n",
    "    # the protected variable is the gender component of the output embedding.\n",
    "    # the extra pair of [] is so that the array has the right shape after\n",
    "    # it is converted to a numpy array.\n",
    "    protect.append([np.dot(embed[analogy[3]], gender_direction)])\n",
    "  # Convert all three to numpy arrays, and return them.\n",
    "  return tuple(map(np.array, (data, labels, protect)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2DeRq6L9BiTo"
   },
   "source": [
    "Edit the training parameters below to experiment with different training runs.\n",
    "\n",
    "For example, try increasing the number of training steps to 50k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19544, 3, 300), (19544, 300), (19544, 1))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy_indices = filter_analogies(analogies, indices)\n",
    "\n",
    "data, labels, protect = make_data(analogy_indices, embed, vec)\n",
    "np.shape(data),np.shape(labels),np.shape(protect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "GhOH62wAR-wz",
    "outputId": "87269f8a-1152-4eec-a16e-9a4190074588"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"var_scope/Mean_1:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-7f70d848648a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     trained_model.fit(sess, data, data_p, labels, labels_p, protect, protect_p, vec,\n\u001b[1;32m     37\u001b[0m               \u001b[0mpred_learning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             protect_learning_rate, protect_loss_weight, num_steps, batch_size)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-67-493a8f826f28>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, sess, data, data_p, labels, labels_p, protect, protect_p, gender_direction, pred_learning_rate, protect_learning_rate, protect_loss_weight, num_steps, batch_size, debug_interval)\u001b[0m\n\u001b[1;32m    183\u001b[0m       \u001b[0munit_protect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotect_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m       \u001b[0;31m# the two lines below can be commented out to train without debiasing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m       \u001b[0mg\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0munit_protect\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0munit_protect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m       \u001b[0mg\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mprotect_loss_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mprotect_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m       \u001b[0mpred_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csweeney/.local/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.pyc\u001b[0m in \u001b[0;36mr_binary_op_wrapper\u001b[0;34m(y, x)\u001b[0m\n\u001b[1;32m    947\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mr_binary_op_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 949\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"x\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    950\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csweeney/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, preferred_dtype)\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreferred_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m       as_ref=False)\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csweeney/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, ctx)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csweeney/.local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.pyc\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    233\u001b[0m                                          as_ref=False):\n\u001b[1;32m    234\u001b[0m   \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/csweeney/.local/lib/python2.7/site-packages/tensorflow/python/framework/constant_op.pyc\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name, verify_shape)\u001b[0m\n\u001b[1;32m    212\u001b[0m   tensor_value.tensor.CopyFrom(\n\u001b[1;32m    213\u001b[0m       tensor_util.make_tensor_proto(\n\u001b[0;32m--> 214\u001b[0;31m           value, dtype=dtype, shape=shape, verify_shape=verify_shape))\n\u001b[0m\u001b[1;32m    215\u001b[0m   \u001b[0mdtype_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattr_value_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttrValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m   const_tensor = g.create_op(\n",
      "\u001b[0;32m/home/csweeney/.local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_util.pyc\u001b[0m in \u001b[0;36mmake_tensor_proto\u001b[0;34m(values, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    419\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"None values not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m     \u001b[0;31m# if dtype is provided, forces numpy array to be the type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;31m# provided if possible.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: None values not supported."
     ]
    }
   ],
   "source": [
    "# Edit the training parameters below to experiment with different training runs.\n",
    "# For example, try \n",
    "pred_learning_rate = 2**-16\n",
    "protect_learning_rate = 2**-16\n",
    "protect_loss_weight = 4\n",
    "num_steps = 30000\n",
    "batch_size = 1000\n",
    "\n",
    "embed_dim = 300\n",
    "projection_dims = 1\n",
    "\n",
    "Y = np.array([targets, -(targets-1)]).T  # The model currently needs one column for each class\n",
    "X, X_test, Y, Y_test = train_test_split(vectors, Y)\n",
    "sess = tf.InteractiveSession()\n",
    "with tf.variable_scope('var_scope', reuse=tf.AUTO_REUSE):\n",
    "    \n",
    "    analogy_indices = filter_analogies(analogies, indices)\n",
    "\n",
    "    data, labels, protect = make_data(analogy_indices, embed, vec)\n",
    "    data_p = tf.placeholder(tf.float32, shape=[None, 3, embed_dim], name=\"data\")\n",
    "    labels_p = tf.placeholder(tf.float32, shape=[None, embed_dim], name=\"labels\")\n",
    "    protect_p = tf.placeholder(tf.float32, shape=[None, 1], name=\"protect\")\n",
    "\n",
    "    # projection is the space onto which we are \"projecting\". By default, this is\n",
    "    # one-dimensional, but this can be tuned by projection_dims\n",
    "    projection = tf.get_variable(\"projection\", [embed_dim, projection_dims])\n",
    "\n",
    "    # build the prediction layer\n",
    "    # pred is the simple computation of d = -a + b + c for a : b :: c : d\n",
    "    pred = -data_p[:, 0, :] + data_p[:, 1, :] + data_p[:, 2, :]\n",
    "    pred -= tf.matmul(tf.matmul(pred, projection), tf.transpose(projection))\n",
    "\n",
    "    trained_model = AdversarialEmbeddingModel(\n",
    "        client, data_p, embed_dim, projection, projection_dims, pred,X,Y,X_test,Y_test)\n",
    "\n",
    "    trained_model.fit(sess, data, data_p, labels, labels_p, protect, protect_p, vec,\n",
    "              pred_learning_rate,\n",
    "            protect_learning_rate, protect_loss_weight, num_steps, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vt--_4UCWTIg"
   },
   "source": [
    "### Analogy generation using the embeddings with bias reduced by the adversarial model\n",
    "\n",
    "Let's see how the model that has been trained to mitigate bias performs on the analogy task.\n",
    "As before, change \"boss\" to \"friend\" to see how those analogies have changed too.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = {}\n",
    "table1 = {}\n",
    "for i in analogies:\n",
    "    for j in i:\n",
    "        if not j in table:\n",
    "            table[j.lower()] = 1\n",
    "            table1[j.lower()] = [i]\n",
    "        else: \n",
    "            table1[j.lower()].append(i)\n",
    "            table[j.lower()]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in nationalities:\n",
    "    if i.lower() in table1:\n",
    "        print table1[i.lower()],i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in nationalities:\n",
    "    if i.lower() in table:\n",
    "        print table[i.lower()],i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w= trained_model.projection.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.similar_by_vector(embeddings['amazing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.similar_by_vector(embeddings['american'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.similar_by_vector(embeddings['american']-np.dot(np.dot(w,w.T),embeddings['american']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words= filter(lambda x: x.lower() in embeddings,nationalities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "w= trained_model.projection.eval()\n",
    "W = np.dot(w,w.T)\n",
    "df = pd.DataFrame(data={\"word\": list(words)})\n",
    "df[\"sentiment_score\"] = df[\"word\"].map(\n",
    "    lambda a: clf.predict_proba([embeddings[a.lower()]-np.dot(np.dot(w,w.T),embeddings[a.lower()])])[0][1])\n",
    "df.sort_values(by=\"sentiment_score\", inplace=True)\n",
    "df.plot.bar(x=\"word\",y=\"sentiment_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w= trained_model.projection.eval()\n",
    "W = np.dot(w,w.T)\n",
    "words = nationalities#filter(lambda x: x.lower() in embeddings,identities)\n",
    "df = pd.DataFrame(data={\"word\": list(words)})\n",
    "df[\"sentiment_score\"] = df[\"word\"].map(\n",
    "    lambda a: (embeddings[a.lower()]-(np.dot(np.dot(w,w.T),embeddings[a.lower()]))).dot(vec))\n",
    "df.sort_values(by=\"sentiment_score\", inplace=True)\n",
    "df.plot.bar(x=\"word\",y=\"sentiment_score\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\"word\": list(words)})\n",
    "df[\"sentiment_score\"] = df[\"word\"].map(\n",
    "    lambda a: embeddings[a.lower()].dot(vec))\n",
    "df.sort_values(by=\"sentiment_score\", inplace=True)\n",
    "df.plot.bar(x=\"word\",y=\"sentiment_score\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = {\"word\": list(words)})\n",
    "df[\"prob_neg_sentiment\"] = df[\"word\"].map(\n",
    "    lambda w: clf.predict_proba([embeddings[w.lower()]])[0][1])\n",
    "df.sort_values(by=\"prob_neg_sentiment\", inplace=True)\n",
    "a= list(df[\"prob_neg_sentiment\"])\n",
    "uniform_dist = np.ones(len(a))*1./len(a)\n",
    "normalized = a/np.sum(a)\n",
    "kl = (normalized * np.log(normalized/uniform_dist)).sum()\n",
    "print kl\n",
    "df.plot.bar(x=\"word\",y=\"prob_neg_sentiment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = {\"word\": list(words)})\n",
    "df[\"prob_neg_sentiment\"] = df[\"word\"].map(\n",
    "    lambda a: clf.predict_proba([(embeddings[a.lower()]-np.dot(np.dot(w,w.T),embeddings[a.lower()]))])[0][1])\n",
    "df.sort_values(by=\"prob_neg_sentiment\", inplace=True)\n",
    "a= list(df[\"prob_neg_sentiment\"])\n",
    "uniform_dist = np.ones(len(a))*1./len(a)\n",
    "normalized = a/np.sum(a)\n",
    "kl = (normalized * np.log(normalized/uniform_dist)).sum()\n",
    "print kl\n",
    "df.plot.bar(x=\"word\",y=\"prob_neg_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identity_toxicity_debias = dict(identity_toxicity_table(nationalities, embeddings,lr))\n",
    "identity_toxicity = dict(identity_toxicity_table(nationalities,embeddings,lr))\n",
    "# keys_common = set(map(lambda x : x[6:] , identity_toxicity_debias.keys())).intersection(set(identity_toxicity.keys()))\n",
    "# identity_toxicity_debias = {key: identity_toxicity_debias[key] for key in keys_common }\n",
    "# identity_toxicity = {key: identity_toxicity[key] for key in keys_common }\n",
    "identity_toxicity_debias = zip(*identity_toxicity_debias.items())\n",
    "identity_toxicity = zip(*identity_toxicity.items())\n",
    "%matplotlib inline\n",
    "f = plt.figure(figsize=(27, 18), dpi= 80, facecolor='w', edgecolor='k')\n",
    "f.subplots_adjust(hspace=1.2)\n",
    "f.add_subplot(411)\n",
    "plt.title(\"GloVe Negative Sentiment Distribution\",fontsize=35)\n",
    "index = np.arange(len(identity_toxicity[0]))\n",
    "plt.bar(index,identity_toxicity[1]/np.sum(identity_toxicity[1]))\n",
    "plt.xticks(index, identity_toxicity[0], fontsize=25, rotation=35)\n",
    "plt.ylim(top=.2,bottom=0)\n",
    "f.add_subplot(412)\n",
    "plt.title(\"GloVe Kernel Negative Sentiment Distribution\",fontsize=35)\n",
    "index = np.arange(len(identity_toxicity_debias[0]))\n",
    "plt.bar(index,identity_toxicity_debias[1]/np.sum(identity_toxicity_debias[1]))\n",
    "plt.xticks(index, identity_toxicity_debias[0], fontsize=25, rotation=35)\n",
    "plt.ylim(top=.2,bottom=0)\n",
    "f.add_subplot(413)\n",
    "plt.title(\"Delta Between Word Embedding Distributions\",fontsize=35)\n",
    "index = np.arange(len(identity_toxicity_debias[0]))\n",
    "delta = identity_toxicity[1]/np.sum(identity_toxicity[1])-identity_toxicity_debias[1]/np.sum(identity_toxicity_debias[1])\n",
    "bar = plt.bar(index,delta)\n",
    "for i,j in enumerate(delta):\n",
    "    bar[i].set_color('r') if j<0 else bar[i].set_color('g')\n",
    "plt.xticks(index, identity_toxicity_debias[0], fontsize=25, rotation=35)\n",
    "plt.ylim(top=.2,bottom=-.06)\n",
    "f.add_subplot(414)\n",
    "plt.title('Fair Uniform Taget Distribution',fontsize=35)\n",
    "index = np.arange(len(identity_toxicity[0]))\n",
    "plt.bar(index,1./len(identity_toxicity[1]))\n",
    "plt.xticks(index, identity_toxicity[0], fontsize=25, rotation=35)\n",
    "plt.ylim(top=.2,bottom=0)\n",
    "plt.show()\n",
    "uniform_dist = np.ones(len(identity_toxicity[1]))*1./len(identity_toxicity[1])\n",
    "uniform_dist_d = np.ones(len(identity_toxicity_debias[1]))*1./len(identity_toxicity_debias[1])\n",
    "\n",
    "debiased_normalized = identity_toxicity_debias[1]/np.sum(identity_toxicity_debias[1])\n",
    "biased_normalized = identity_toxicity[1]/np.sum(identity_toxicity[1])\n",
    "kl_debiased = (debiased_normalized * np.log(debiased_normalized/uniform_dist_d)).sum()\n",
    "kl_biased = (biased_normalized * np.log(biased_normalized/uniform_dist)).sum()\n",
    "print 'kl divergence from uniform dist for concept net:',kl_debiased\n",
    "print 'kl divergence from uniform dist for google news:',kl_biased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "_56sPusFUVQP",
    "outputId": "e1b2db68-f87f-4123-aaab-d094302f6c5c"
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "A = \"american\"\n",
    "B = \"mexican\"\n",
    "C = \"good\"\n",
    "NUM_ANALOGIES = 5\n",
    "\n",
    "# Use a word embedding to compute an analogy\n",
    "in_arr = []\n",
    "for i, word in enumerate((A, B, C)):\n",
    "  in_arr.append(client.word_vec(word))\n",
    "in_arr = np.array([in_arr])\n",
    "\n",
    "print_knn(client, sess.run(pred, feed_dict={data_p: in_arr}),\n",
    "          NUM_ANALOGIES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Ei_ioWT_DDa"
   },
   "source": [
    "##Conclusion\n",
    "\n",
    "The method demonstrated here helps to reduce the amount of bias in word embeddings and, although not demonstrated here, generalizes quite well to other domains and tasks.  By trying to hide a protected variable from an adversary, a machine learned system can reduce the amount of biased information about that protected variable implicit in the system.  In addition to the specific method demonstrated here there are many variations on this theme which can be used to achieve different degrees and types of debiasing.  For example, you could debias with respect to more than one principle component of the protected variable by having the adverary predict multiple projections.  Many other elaborations on this basic idea are possible and hopefully this relatively simple system can serve as the basis for more complex and sophisticated systems capable of achieving subtle types of bias mitigation in many applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array([targets, -(targets-1)]).T  # The model currently needs one column for each class\n",
    "X, X_test, Y, Y_test = train_test_split(vectors, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 10 # 1st layer number of features\n",
    "n_hidden_2 = 10 # 2nd layer number of features\n",
    "n_input = 300 # Number of feature\n",
    "n_classes = 2 # Number of classes to predict\n",
    "\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Launch the graph\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(len(X)/batch_size)\n",
    "        X_batches = np.array_split(X, total_batch)\n",
    "        Y_batches = np.array_split(Y, total_batch)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = X_batches[i], Y_batches[i]\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "                                                          y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: X_test, y: Y_test}))\n",
    "    global result \n",
    "    result = tf.argmax(pred, 1).eval({x: X_test, y: Y_test})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "JndnmDMp66FL"
   ],
   "name": "Mitigating Unwanted Biases in Word Embeddings with Adversarial Learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
