{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except ImportError as error:\n",
    "    print(\"Import error: %s\" % (error))\n",
    "\n",
    "from aif360.algorithms import Transformer\n",
    "\n",
    "\n",
    "class AdversarialDebiasing(Transformer):\n",
    "    \"\"\"Adversarial debiasing is an in-processing technique that learns a\n",
    "    classifier to maximize prediction accuracy and simultaneously reduce an\n",
    "    adversary's ability to determine the protected attribute from the\n",
    "    predictions [5]_. This approach leads to a fair classifier as the\n",
    "    predictions cannot carry any group discrimination information that the\n",
    "    adversary can exploit.\n",
    "\n",
    "    References:\n",
    "        .. [5] B. H. Zhang, B. Lemoine, and M. Mitchell, \"Mitigating Unwanted\n",
    "           Biases with Adversarial Learning,\" AAAI/ACM Conference on Artificial\n",
    "           Intelligence, Ethics, and Society, 2018.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 unprivileged_groups,\n",
    "                 privileged_groups,\n",
    "                 scope_name,\n",
    "                 sess,\n",
    "                 seed=None,\n",
    "                 adversary_loss_weight=0.1,\n",
    "                 num_epochs=50,\n",
    "                 batch_size=128,\n",
    "                 classifier_num_hidden_units=200,\n",
    "                 debias=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            unprivileged_groups (tuple): Representation for unprivileged groups\n",
    "            privileged_groups (tuple): Representation for privileged groups\n",
    "            scope_name (str): scope name for the tenforflow variables\n",
    "            sess (tf.Session): tensorflow session\n",
    "            seed (int, optional): Seed to make `predict` repeatable.\n",
    "            adversary_loss_weight (float, optional): Hyperparameter that chooses\n",
    "                the strength of the adversarial loss.\n",
    "            num_epochs (int, optional): Number of training epochs.\n",
    "            batch_size (int, optional): Batch size.\n",
    "            classifier_num_hidden_units (int, optional): Number of hidden units\n",
    "                in the classifier model.\n",
    "            debias (bool, optional): Learn a classifier with or without\n",
    "                debiasing.\n",
    "        \"\"\"\n",
    "        super(AdversarialDebiasing, self).__init__(\n",
    "            unprivileged_groups=unprivileged_groups,\n",
    "            privileged_groups=privileged_groups)\n",
    "\n",
    "        self.scope_name = scope_name\n",
    "        self.seed = seed\n",
    "\n",
    "        self.unprivileged_groups = unprivileged_groups\n",
    "        self.privileged_groups = privileged_groups\n",
    "        if len(self.unprivileged_groups) > 1 or len(self.privileged_groups) > 1:\n",
    "            raise ValueError(\"Only one unprivileged_group or privileged_group supported.\")\n",
    "        self.protected_attribute_name = list(self.unprivileged_groups[0].keys())[0]\n",
    "\n",
    "        self.sess = sess\n",
    "        self.adversary_loss_weight = adversary_loss_weight\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.classifier_num_hidden_units = classifier_num_hidden_units\n",
    "        self.debias = debias\n",
    "\n",
    "        self.features_dim = None\n",
    "        self.features_ph = None\n",
    "        self.protected_attributes_ph = None\n",
    "        self.true_labels_ph = None\n",
    "        self.pred_labels = None\n",
    "\n",
    "    def _classifier_model(self, features, features_dim, keep_prob):\n",
    "        \"\"\"Compute the classifier predictions for the outcome variable.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"classifier_model\"):\n",
    "            W1 = tf.get_variable('W1', [features_dim, self.classifier_num_hidden_units],\n",
    "                                  initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b1 = tf.Variable(tf.zeros(shape=[self.classifier_num_hidden_units]), name='b1')\n",
    "\n",
    "            h1 = tf.nn.relu(tf.matmul(features, W1) + b1)\n",
    "            h1 = tf.nn.dropout(h1, keep_prob=keep_prob)\n",
    "\n",
    "            W2 = tf.get_variable('W2', [self.classifier_num_hidden_units, 1],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.zeros(shape=[1]), name='b2')\n",
    "\n",
    "            pred_logit = tf.matmul(h1, W2) + b2\n",
    "            pred_label = tf.sigmoid(pred_logit)\n",
    "\n",
    "        return pred_label, pred_logit\n",
    "\n",
    "    def _adversary_model(self, pred_logits, true_labels):\n",
    "        \"\"\"Compute the adversary predictions for the protected attribute.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(\"adversary_model\"):\n",
    "            c = tf.get_variable('c', initializer=tf.constant(1.0))\n",
    "            s = tf.sigmoid((1 + tf.abs(c)) * pred_logits)\n",
    "\n",
    "            W2 = tf.get_variable('W2', [3, 1],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b2 = tf.Variable(tf.zeros(shape=[1]), name='b2')\n",
    "\n",
    "            pred_protected_attribute_logit = tf.matmul(tf.concat([s, s * true_labels, s * (1.0 - true_labels)], axis=1), W2) + b2\n",
    "            pred_protected_attribute_label = tf.sigmoid(pred_protected_attribute_logit)\n",
    "\n",
    "        return pred_protected_attribute_label, pred_protected_attribute_logit\n",
    "\n",
    "    def fit(self, dataset):\n",
    "        \"\"\"Compute the model parameters of the fair classifier using gradient\n",
    "        descent.\n",
    "\n",
    "        Args:\n",
    "            dataset (BinaryLabelDataset): Dataset containing true labels.\n",
    "\n",
    "        Returns:\n",
    "            AdversarialDebiasing: Returns self.\n",
    "        \"\"\"\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        # Map the dataset labels to 0 and 1.\n",
    "        temp_labels = dataset.labels.copy()\n",
    "\n",
    "        temp_labels[(dataset.labels == dataset.favorable_label).ravel(),0] = 1.0\n",
    "        temp_labels[(dataset.labels == dataset.unfavorable_label).ravel(),0] = 0.0\n",
    "\n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            num_train_samples, self.features_dim = np.shape(dataset.features)\n",
    "\n",
    "            # Setup placeholders\n",
    "            self.features_ph = tf.placeholder(tf.float32, shape=[None, self.features_dim])\n",
    "            self.protected_attributes_ph = tf.placeholder(tf.float32, shape=[None,1])\n",
    "            self.true_labels_ph = tf.placeholder(tf.float32, shape=[None,1])\n",
    "            self.keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "            # Obtain classifier predictions and classifier loss\n",
    "            self.pred_labels, pred_logits = self._classifier_model(self.features_ph, self.features_dim, self.keep_prob)\n",
    "            pred_labels_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.true_labels_ph, logits=pred_logits))\n",
    "\n",
    "            if self.debias:\n",
    "                # Obtain adversary predictions and adversary loss\n",
    "                pred_protected_attributes_labels, pred_protected_attributes_logits = self._adversary_model(pred_logits, self.true_labels_ph)\n",
    "                pred_protected_attributes_loss = tf.reduce_mean(\n",
    "                    tf.nn.sigmoid_cross_entropy_with_logits(labels=self.protected_attributes_ph, logits=pred_protected_attributes_logits))\n",
    "\n",
    "            # Setup optimizers with learning rates\n",
    "            global_step = tf.Variable(0, trainable=False)\n",
    "            starter_learning_rate = 0.001\n",
    "            learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                                       1000, 0.96, staircase=True)\n",
    "            classifier_opt = tf.train.AdamOptimizer(learning_rate)\n",
    "            if self.debias:\n",
    "                adversary_opt = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "            classifier_vars = [var for var in tf.trainable_variables() if 'classifier_model' in var.name]\n",
    "            if self.debias:\n",
    "                adversary_vars = [var for var in tf.trainable_variables() if 'adversary_model' in var.name]\n",
    "                # Update classifier parameters\n",
    "                adversary_grads = {var: grad for (grad, var) in adversary_opt.compute_gradients(pred_protected_attributes_loss,\n",
    "                                                                                      var_list=classifier_vars)}\n",
    "            normalize = lambda x: x / (tf.norm(x) + np.finfo(np.float32).tiny)\n",
    "\n",
    "            classifier_grads = []\n",
    "            for (grad,var) in classifier_opt.compute_gradients(pred_labels_loss, var_list=classifier_vars):\n",
    "                if self.debias:\n",
    "                    unit_adversary_grad = normalize(adversary_grads[var])\n",
    "                    grad -= tf.reduce_sum(grad * unit_adversary_grad) * unit_adversary_grad\n",
    "                    grad -= self.adversary_loss_weight * adversary_grads[var]\n",
    "                classifier_grads.append((grad, var))\n",
    "            classifier_minimizer = classifier_opt.apply_gradients(classifier_grads, global_step=global_step)\n",
    "\n",
    "            if self.debias:\n",
    "                # Update adversary parameters\n",
    "                adversary_minimizer = adversary_opt.minimize(pred_protected_attributes_loss, var_list=adversary_vars, global_step=global_step)\n",
    "\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            self.sess.run(tf.local_variables_initializer())\n",
    "\n",
    "            # Begin training\n",
    "            for epoch in range(self.num_epochs):\n",
    "                shuffled_ids = np.random.choice(num_train_samples, num_train_samples)\n",
    "                for i in range(num_train_samples//self.batch_size):\n",
    "                    batch_ids = shuffled_ids[self.batch_size*i: self.batch_size*(i+1)]\n",
    "                    batch_features = dataset.features[batch_ids]\n",
    "                    batch_labels = np.reshape(temp_labels[batch_ids], [-1,1])\n",
    "                    batch_protected_attributes = np.reshape(dataset.protected_attributes[batch_ids][:,\n",
    "                                                 dataset.protected_attribute_names.index(self.protected_attribute_name)], [-1,1])\n",
    "\n",
    "                    batch_feed_dict = {self.features_ph: batch_features,\n",
    "                                       self.true_labels_ph: batch_labels,\n",
    "                                       self.protected_attributes_ph: batch_protected_attributes,\n",
    "                                       self.keep_prob: 0.8}\n",
    "                    if self.debias:\n",
    "                        _, _, pred_labels_loss_value, pred_protected_attributes_loss_vale = self.sess.run([classifier_minimizer,\n",
    "                                       adversary_minimizer,\n",
    "                                       pred_labels_loss,\n",
    "                                       pred_protected_attributes_loss], feed_dict=batch_feed_dict)\n",
    "                        if i % 200 == 0:\n",
    "                            print(\"epoch %d; iter: %d; batch classifier loss: %f; batch adversarial loss: %f\" % (epoch, i, pred_labels_loss_value,\n",
    "                                                                                     pred_protected_attributes_loss_vale))\n",
    "                    else:\n",
    "                        _, pred_labels_loss_value = self.sess.run(\n",
    "                            [classifier_minimizer,\n",
    "                             pred_labels_loss], feed_dict=batch_feed_dict)\n",
    "                        if i % 200 == 0:\n",
    "                            print(\"epoch %d; iter: %d; batch classifier loss: %f\" % (\n",
    "                            epoch, i, pred_labels_loss_value))\n",
    "        return self\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        \"\"\"Obtain the predictions for the provided dataset using the fair\n",
    "        classifier learned.\n",
    "\n",
    "        Args:\n",
    "            dataset (BinaryLabelDataset): Dataset containing labels that needs\n",
    "                to be transformed.\n",
    "        Returns:\n",
    "            dataset (BinaryLabelDataset): Transformed dataset.\n",
    "        \"\"\"\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        num_test_samples, _ = np.shape(dataset.features)\n",
    "\n",
    "        samples_covered = 0\n",
    "        pred_labels = []\n",
    "        while samples_covered < num_test_samples:\n",
    "            start = samples_covered\n",
    "            end = samples_covered + self.batch_size\n",
    "            if end > num_test_samples:\n",
    "                end = num_test_samples\n",
    "            batch_ids = np.arange(start, end)\n",
    "            batch_features = dataset.features[batch_ids]\n",
    "            batch_labels = np.reshape(dataset.labels[batch_ids], [-1,1])\n",
    "            batch_protected_attributes = np.reshape(dataset.protected_attributes[batch_ids][:,\n",
    "                                         dataset.protected_attribute_names.index(self.protected_attribute_name)], [-1,1])\n",
    "\n",
    "            batch_feed_dict = {self.features_ph: batch_features,\n",
    "                               self.true_labels_ph: batch_labels,\n",
    "                               self.protected_attributes_ph: batch_protected_attributes,\n",
    "                               self.keep_prob: 1.0}\n",
    "\n",
    "            pred_labels += self.sess.run(self.pred_labels, feed_dict=batch_feed_dict)[:,0].tolist()\n",
    "            samples_covered += len(batch_features)\n",
    "\n",
    "        # Mutated, fairer dataset with new labels\n",
    "        dataset_new = dataset.copy(deepcopy = True)\n",
    "        dataset_new.labels = (np.array(pred_labels)>0.5).astype(np.float64).reshape(-1,1)\n",
    "\n",
    "        # Map the dataset labels to back to their original values.\n",
    "        temp_labels = dataset_new.labels.copy()\n",
    "\n",
    "        temp_labels[(dataset_new.labels == 1.0).ravel(), 0] = dataset.favorable_label\n",
    "        temp_labels[(dataset_new.labels == 0.0).ravel(), 0] = dataset.unfavorable_label\n",
    "\n",
    "        dataset_new.labels = temp_labels.copy()\n",
    "\n",
    "        return dataset_new\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
